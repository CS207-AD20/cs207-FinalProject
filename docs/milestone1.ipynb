{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "**1. Introduction to Automatic Differentiation**\n",
    "\n",
    "    1.1 Problem\n",
    "    1.2 Why are we worried about Automatic Differentiation?\n",
    "    \n",
    "**2. Background**\n",
    "\n",
    "    2.1 Finite Difference\n",
    "    2.2 Chain Rule\n",
    "    2.3 Computational Graph Structure\n",
    "    2.4 Review of Complex Numbers\n",
    "    2.5 Dual Numbers\n",
    "    2.6 Elementary Functions\n",
    "    \n",
    "**3. How to Use**\n",
    "\n",
    "    3.1 How to interact with the package\n",
    "    3.2 How to import the package\n",
    "    3.3 How to instantiate AD object\n",
    "    \n",
    "**4. Software Organization**\n",
    "\n",
    "    4.1 What will the directory structure look like?\n",
    "    4.2 What modules do you plan on including? What is their basic functionality?\n",
    "    4.3 Where will your test suite live? Will you use TravisCI? Coveralls?\n",
    "    4.4 How will you distribute your package (e.g. PyPI)?\n",
    "    \n",
    "**5. Implementation**\n",
    "\n",
    "    5.1 What are the core data structures?\n",
    "    5.2 What classes will you implement?\n",
    "    5.3 What method and name attributes will your classes have?\n",
    "    5.4 What external dependencies will you rely on?\n",
    "    5.5 How will you deal with elementary functions like sin and exp?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Automatic differentiation (AD), also called algorithmic differentiation or computational differentiation, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    "\n",
    "## 1.1 Problem\n",
    "\n",
    "Differentiation is one of the most important operations in science.  Finding extrema of functions and determining zeros of functions are central to optimization.  Numerically solving differential equations forms a cornerstone of modern science and engineering and is intimately linked with predictive science. However, people commonly misunderstand AD to be similar with Numerical Differentiation or Symbolic Differentiation.\n",
    "\n",
    "### Not Numerical Differentiation\n",
    "Numerical differentiation is the technique one would obviously think of from the standard definition of a derivative. With\n",
    "\n",
    "$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h)−f(x)}{h}$,\n",
    "\n",
    "you can clearly approximate the left hand side by evaluating the right hand side at a small but nonzero h. This has the advantage of being blindingly easy to code, but the disadvantages of costing O(n) evaluations of f for gradients in n dimensions, and of catching you between the rock of truncation error and the hard place of roundoff error. Techniques have of course been developed to mitigate these problems, but these techniques increase rapidly in programming complexity.\n",
    "\n",
    "This approach is nice because it is quick and easy. However, it suffers from accuracy and stability problems.\n",
    "\n",
    "### Not Symbolic Differentiation\n",
    "On the other hand, symbolic derivatives can be evaluated to machine precision, but can be costly to evaluate. We'll have more to say about cost of symbolic differentiation later.\n",
    "\n",
    "Automatic differentiation (AD) overcomes both of these deficiencies. It is less costly than symbolic differentiation while evaluating derivatives to machine precision.  \n",
    "\n",
    "## 1.2 Why\n",
    "\n",
    "There are two modes of automatic differentiation: forward and reverse.  This package will be primarily concerned with the forward mode. Time-permitting, we will give an introduction to the reverse mode.  In fact, the famous backpropagation algorithm from machine learn is a special case of the reverse mode of automatic differentiation.\n",
    "\n",
    "In conclusion, any derivative or gradient, of any function you can program, or of any program that computes a function, with machine accuracy and ideal asymptotic efficiency. Some sample use cases may be as follows:\n",
    "\n",
    "- Real-parameter optimization (many good methods are gradient-based)\n",
    "- Sensitivity analysis (local sensitivity = ∂(result)/∂(input))\n",
    "- Physical modeling (forces are derivatives of potentials; equations of motion are derivatives of Lagrangians and Hamiltonians; etc)\n",
    "- Probabilistic inference (e.g., Hamiltonian Monte Carlo)\n",
    "- Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background\n",
    "\n",
    "## 2.1 The Finite Difference\n",
    "Suppose we want to avoid relying on the symbolic computation of the derivative.  An obvious and very convenient way to do so is to use a finite difference.  For a single-variable function, we just write $$\\dfrac{\\partial f}{\\partial x} \\approx \\dfrac{f\\left(x+\\epsilon\\right) - f\\left(x\\right)}{\\epsilon}$$ for some \"small\" $\\epsilon$.  Let's do a little demo to see how things turn out.\n",
    "\n",
    "We'll compute the derivative of $$f\\left(x\\right) = x - \\exp\\left(-2\\sin^{2}\\left(4x\\right)\\right).$$  We've already defined `python` functions for $f\\left(x\\right)$ and its derivative.  Let's define a function for the first order finite difference.\n",
    "\n",
    "The finite difference approach is nice because it is quick and easy.  However, it suffers from accuracy and stability problems. \n",
    "\n",
    "## 2.2 Chain Rule\n",
    "\n",
    "### Review of the Chain Rule\n",
    "\n",
    "At the heart of AD is the famous *chain rule* from calculus.\n",
    "\n",
    "### Back to the Beginning\n",
    "Suppose we have a function $h\\left(u\\left(t\\right)\\right)$ and we want the derivative of $h$ with respect to $t$.  The derivative is $$\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$$  This is the rule that we used in symbolically computing the derivative of the function $f\\left(x\\right) = x - \\exp\\left(-2\\sin^{2}\\left(4x\\right)\\right)$ earlier.\n",
    "\n",
    "### Adding an Argument\n",
    "Now suppose $h$ has another argument so that we have $h\\left(u\\left(t\\right),v\\left(t\\right)\\right)$.  Once again, we want the derivative of $h$ with respect to $t$.  Applying the chain rule in this case gives\n",
    "\\begin{align}\n",
    "  \\displaystyle \n",
    "  \\frac{\\partial h}{\\partial t} = \\frac{\\partial h}{\\partial u}\\frac{\\partial u}{\\partial t} + \\frac{\\partial h}{\\partial v}\\frac{\\partial v}{\\partial t}.\n",
    "\\end{align}\n",
    "\n",
    "### The Gradient\n",
    "What if we replace $t$ by a vector $x\\in\\mathbb{R}^{m}$?  Now we want the gradient of $h$ with respect to $x$.  We write $h = h\\left(u\\left(x\\right),v\\left(x\\right)\\right)$ and the derivative is now \n",
    "\\begin{align}\n",
    "  \\nabla_{x} h = \\frac{\\partial h}{\\partial u}\\nabla u + \\frac{\\partial h}{\\partial v} \\nabla v\n",
    "\\end{align}\n",
    "where we have written $\\nabla_{x}$ on the left hand side to avoid any confusion with arguments.  The gradient operator on the right hand side is clearly with respect to $x$ since $u$ and $v$ have no other arguments.\n",
    "\n",
    "### The General Rule\n",
    "In general $h = h\\left(y\\left(x\\right)\\right)$ where $y\\in\\mathbb{R}^{n}$ and $x\\in\\mathbb{R}^{m}$.  Now $h$ is a function of possibly $n$ other functions themselves a function of $m$ variables.  The gradient of $h$ is now given by \n",
    "\\begin{align}\n",
    "  \\nabla_{x}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## 2.3 Graph Structure on Calculations\n",
    "\n",
    "### The Computational Graph\n",
    "Consider again the example function $$f\\left(x\\right) = x - \\exp\\left(-2\\sin^{2}\\left(4x\\right)\\right).$$  We'd like to evalute $f$ at the point $a$.  In the graph, we indicate the input value as $x$ and the output value as $f$.  Note that $x$ should take on whatever value you want it to.\n",
    "\n",
    "Let's find $f\\left(\\dfrac{\\pi}{16}\\right)$.  The evaluation trace looks like:\n",
    "\n",
    "| Trace    | Elementary Operation                   | Numerical Value                  |\n",
    "| :------: | :----------------------:               | :------------------------------: |\n",
    "| $x_{1}$  | $\\dfrac{\\pi}{16}$                      | $\\dfrac{\\pi}{16}$                |\n",
    "| $x_{2}$  | $4x_{1}$                               | $\\dfrac{\\pi}{4}$                 |\n",
    "| $x_{3}$  | $\\sin\\left(x_{2}\\right)$               | $\\dfrac{\\sqrt{2}}{2}$            |\n",
    "| $x_{4}$  | $x_{3}^{2}$                            | $\\dfrac{1}{2}$                   |\n",
    "| $x_{5}$  | $-2x_{4}$                              | $-1$                             |\n",
    "| $x_{6}$  | $\\exp\\left(x_{5}\\right)$               | $\\dfrac{1}{e}$                   |\n",
    "| $x_{7}$  | $-x_{6}$                               | $-\\dfrac{1}{e}$                  |\n",
    "| $x_{8}$  | $x_{1} + x_{7}$                        | $\\dfrac{\\pi}{16} - \\dfrac{1}{e}$ |\n",
    "\n",
    "Of course, the computer holds floating point values.  The value of $f\\left(\\dfrac{\\pi}{16}\\right)$ is $-0.17152990032208026$.  We can check this with our function.\n",
    "\n",
    "One way to visualize what is going on is to represent the evaluation trace with a graph.\n",
    "\n",
    "![comp-graph](figs/Computational-Graph.png)\n",
    "\n",
    "## 2.4 Review of Comlex Numbers\n",
    "Recall that a complex number has the form $$z = a + ib$$ where we *define* the number $i$ so that $i^{2} = -1$.  No real number has this property but it is a useful property for a number to have.  Hence the introduction of complex numbers.  Visually, you can think of a real number as a number lying on a straight line.  Then, we \"extend\" the real line \"up\".  The new axis is called the *imaginary* axis.\n",
    "\n",
    "Complex numbers have several properties that we can use.\n",
    "* Complex conjugate: $z^{*} = a - ib$.\n",
    "* Magnitude of a complex number: $\\left|z\\right|^{2} = zz^{*} = \\left(a+ib\\right)\\left(a-ib\\right) = a^{2} + b^{2}$.\n",
    "* Polar form: $z = \\left|z\\right|\\exp\\left(i\\theta\\right)$ where $\\displaystyle \\theta = \\tan^{-1}\\left(\\dfrac{b}{a}\\right)$.\n",
    "\n",
    "## 2.5 Dual Numbers\n",
    "A dual number has a real part and a dual part.  We write $$z = x + \\epsilon x^{\\prime}$$ and refer to $x^{\\prime}$ as the dual part.  We *define* the number $\\epsilon$ so that $\\epsilon^{2} = 0$.  **This does not mean that $\\epsilon$ is zero!**  $\\epsilon$ is not a real number.\n",
    "\n",
    "#### Some properties of dual numbers:\n",
    "* Conjugate:  $z^{*} = x - \\epsilon x^{\\prime}$.\n",
    "* Magnitude: $\\left|z\\right|^{2} = zz^{*} = \\left(x+\\epsilon x^{\\prime}\\right)\\left(x-\\epsilon x^{\\prime}\\right) = x^{2}$.\n",
    "* Polar form: $z = x\\left(1 + \\dfrac{x^{\\prime}}{x}\\right)$.\n",
    "\n",
    "### Example\n",
    "Recall that the derivative of $y=x^{2}$ is $y^{\\prime} = 2xx^{\\prime} = 2x$.\n",
    "\n",
    "Now if we extend $x$ so that it has a real part and a dual part ($x\\leftarrow x + \\epsilon x^{\\prime}$) and evaluate $y$ we have\n",
    "\\begin{align}\n",
    "  y &= \\left(x + \\epsilon x^{\\prime}\\right)^{2} \\\\\n",
    "    &= x^{2} + 2xx^{\\prime}\\epsilon + \\underbrace{x^{\\prime^{2}}\\epsilon^{2}}_{=0} \\\\\n",
    "    &= x^{2} + 2xx^{\\prime}\\epsilon.\n",
    "\\end{align}\n",
    "#### Notice that the dual part contains the derivative of our function!!\n",
    "\n",
    "### Example\n",
    "Evaluate $y = \\sin\\left(x\\right)$ when $x\\leftarrow x + \\epsilon x^{\\prime}$.\n",
    "\n",
    "We have\n",
    "\\begin{align}\n",
    "  y & = \\sin\\left(x + \\epsilon x^{\\prime}\\right) \\\\\n",
    "    & = \\sin\\left(x\\right)\\cos\\left(\\epsilon x^{\\prime}\\right) + \\cos\\left(x\\right)\\sin\\left(\\epsilon x^{\\prime}\\right).\n",
    "\\end{align}\n",
    "Expanding $\\cos$ and $\\sin$ in their Taylor series gives \n",
    "\\begin{align}\n",
    "  \\sin\\left(\\epsilon x^{\\prime}\\right) &= \\sum_{n=0}^{\\infty}{\\left(-1\\right)^{n}\\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{2n+1}}{\\left(2n+1\\right)!}} = \\epsilon x^{\\prime} + \\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{3}}{3!} + \\cdots = \\epsilon x^{\\prime} \\\\\n",
    "  \\cos\\left(\\epsilon x^{\\prime}\\right) &= \\sum_{n=0}^{\\infty}{\\left(-1\\right)^{n}\\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{2n}}{\\left(2n\\right)!}} = 1 + \\dfrac{\\left(\\epsilon x^{\\prime}\\right)^{2}}{2} + \\cdots = 1.\n",
    "\\end{align}\n",
    "Note that the definition of $\\epsilon$ was used which resulted in the collapsed sum.\n",
    "\n",
    "So we see that \n",
    "\\begin{align}\n",
    "  y & = \\sin\\left(x\\right) + \\cos\\left(x\\right) x^{\\prime} \\epsilon.\n",
    "\\end{align}\n",
    "And once again the real component is the function and the dual component is the derivative.\n",
    "\n",
    "\n",
    "## 2.6 Elementary Functions\n",
    "Any complex equation can be broken into combinations of the elementary functions. Some of those include the elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, tan, sqrt etc.). We will not go into details about how to calculate the derivatives of those functions here, but more information can be found on the following link.\n",
    "\n",
    "http://www.nabla.hr/FU-DerivativeA5.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How to Use\n",
    "\n",
    "## 3.1 How to interact?\n",
    "Users should use ADnum objects to wrap up all mathematical meaning values and formulas. All operations are processed as an ADnum object. Users need to create an ADnum object for each input variable and use all the mathematical functions defined in the ADmath library to implement special functions.\n",
    "\n",
    "## 3.2 How to import?\n",
    "\timport AD20\n",
    "or \n",
    "\tfrom AD20 import ADnum\n",
    "\tfrom AD20 import ADmath\n",
    "\tfrom AD20 import ADgraph\n",
    "\n",
    "\n",
    "## 3.3 How to instantiate AD\n",
    "\tfrom AD20 import ADnum\n",
    "\tfrom AD20 import ADmath\n",
    "\ta = ADnum(2)\n",
    "\tb = ADmath.sin(a)\n",
    "\t\n",
    "Both a and b are ADnum objects, which have the attributes described in the class implementation below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Software Organization\n",
    "We would like to let the user use all numerical operations defined in our AD20 package. Within AD20 package, there is ADnum module, ADmath module  and ADgraph module\n",
    "\n",
    "For either a scalar or vector input (either as a numpy array or a list), we will convert the input into an ADnum object, which can interact with the other modules. ADnum will also contain an overloaded version of basic operations, including addition, subtraction, multiplication, division, and exponentiation, so that the value and derivative are correctly updated.\n",
    "\n",
    "For special functions, we will use ADmath to compute the numerical values and the corresponding derivatives. In particular, ADmath will contain functions abs, exp, log, sin, cos, and tan.\n",
    "\n",
    "To show a calculation graph, we use ADgraph (and ADtable) to show the forward mode calculation process.\n",
    "\n",
    "###  4.1 What will the directory structure look like?\n",
    "    AD20/\n",
    "        AD20/\n",
    "            __init__.py\n",
    "                ADnum/\n",
    "                    __init__.py\n",
    "                    ADnum.py\n",
    "                ADmath/\n",
    "                    __init__.py\n",
    "                    ADmath.py\n",
    "                ADgraph/\n",
    "                    __init__.py\n",
    "                    ADgraph.py\n",
    "                    ADtable.py\n",
    "        Tests/\n",
    "            __init__.py\n",
    "            test_AD20.py\n",
    "    README.md\n",
    "    setup.py\n",
    "    LICENSE\n",
    "\n",
    "###  4.2 What modules do you plan on including? What is their basic functionality?\n",
    "ADnum: wrap numbers or tuples as a AD object. Moreover, do all of the numerical operations and keep track of all derivatives\n",
    "ADmath: assign special math meanings and functions to ADnum’s and keep track of the derivatives\n",
    "ADgraph: trace the calculation process and generate table or graph\n",
    "\n",
    "In particular, these modules contain the following:\n",
    "ADnum.py contains the class for ADnum.  This class is fully described below.  It takes as input a single scalar input or a vector input (as either a numpy array or list) and outputs an ADnum object.  Within this class, we will overload basic operations as outlined below.\n",
    "\n",
    "###  4.3 Where will your test suite live? Will you use TravisCI? Coveralls?\n",
    "The tests will be stored in the tests directory (see the repo structure above).  We will use pytest to perform our testing, using TravisCI and Coveralls for continuous integration and verifying code coverage respectively.\n",
    "\n",
    "###  4.4 How will you distribute your package (e.g. PyPI)?\n",
    "We will use PIP in PyPi to distribute our package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementation\n",
    "Automatic differentiation will be implemented through the use of ADnum objects and building the functions for which we want to take derivatives from these ADnum objects as well as the special functions defined for ADnum objects in the ADmath module.  Each of these functions is itself an ADnum object so has an associated value and derivative which was updated when constructing the ADnum object through basic operations and special functions.\n",
    "\n",
    "### 5.1 What are the core data structures?\n",
    "The main data structure used to represent the functions on which we are performing automatic differentiation will be tuples, with the first entry the value of the ADnum object and the second entry its derivative.  In the case of scalar input, the derivative is also a float.  For vector valued input, the derivative is the gradient of the function, stored as a numpy array.\n",
    "In order to build and store computational graphs, we will use a dictionary as the computational graph, where the keys are the nodes of the graph, stored as ADnum objects, and the values associated with each key are the children of that node, stored as lists of ADnum objects.\n",
    "\n",
    "### 5.2 What classes will you implement?\n",
    "The main class will be implemented in the ADnum module, which will create ADnum objects.  The ADnum objects will store the current value of the function and its derivative as attributes.  By combining simple ADnum objects with basic operations and simple functions, we can construct any function we like.  For example,\n",
    "\n",
    "    X = AD20.ADnum(4)\n",
    "    Y = AD20.ADnum(0)\n",
    "    F = X+ADmath.sin(Y)\n",
    "    \n",
    "Where F is now an ADnum object, and ADmath.sin() is a specially defined sine function which takes as input an ADnum object and returns an ADnum object, which allows us to evaluate F and its derivative,\n",
    "\n",
    "    F.val = 4\n",
    "    F.deriv = [1, 1] \n",
    "    X.val = 4\n",
    "    X.deriv = 1\n",
    "\n",
    "In addition to the sine function, the ADmath module will also implement the other trigonometric functions, the natural exponential, and the natural logarithm.\n",
    "\n",
    "We will also implement a class, ADgraph, for computational graphs.  The constructor takes as input a dictionary, as described above where the keys are nodes and values are the children of the key node. \tThis can then be used to perform forward propagation and could be extended later to include back propagation as an extension of our project.\n",
    " \n",
    "### 5.3 What method and name attributes will your classes have?\n",
    "Each ADnum object will have two attributes for the two major functions desired of the class.  The val attribute will be the ADnum object evaluated at the given value and the der attribute will be its derivative.  In addition, each ADnum object will have a graph attribute, which stores the dictionary which can be used to build a computational graph in the ADgraph class.  The ADnum class will also include methods to overload basic operations, __add__(), __radd__(), __mul__(), __rmul__(), __sub__(), __truedivide__(), and __pow__().  The result of overloading is that the adding, subtracting, multiplying, dividing, or exponentiating two ADnum objects returns an ADnum object as well as addition or multiplication by a constant.  For example, Y1, Y2, and Y3 would all be recognized as ADnum objects:\n",
    "\n",
    "    X1= ADnum(7)\n",
    "    X2 = ADnum(15)\n",
    "    Y1 = X1+X2\n",
    "    Y2 = X1*X2+X1\n",
    "    Y3 = 5*X1+X2+100\n",
    "\n",
    "The resulting ADnum objects have both a value and derivative.\n",
    "\n",
    "The ADgraph class will be constructed from a dictionary, stored in the attribute dict.  This class will also have an attribute inputs, which stores the nodes which have no parents.  This class will implement a deriv method which returns the derivative from the computational graph.\n",
    "\n",
    "### 5.4 What external dependencies will you rely on?\n",
    "In order to implement the elementary functions, our class will rely on numpy’s implementation of the trigonometric functions, exponential functions, and natural logarithms for evaluation of these special functions.\n",
    "\n",
    "We will also use numpy to implement matrix and vector multiplication in cases where the function is either vector valued or takes a vector as an input.\n",
    "\n",
    "### 5.5 How will you deal with elementary functions like sin and exp?\n",
    "As outlined above, we will have a special ADmath module which defines the trigonometric, exponential, and logarithmic functions to be used on ADnum objects, so that they both take as input and return an ADnum object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
