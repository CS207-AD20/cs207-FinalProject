{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AD20 Milestone 2\n",
    "Group 20: Lindsey Brown, Xinyue Wang, Kevin Yoon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "**1. Introduction**\n",
    "\n",
    "    1.1 Automatic Differentiation as a Solution to the Problem of Computing Derivatives\n",
    "    1.2 Application of AD Techniques\n",
    "    \n",
    "**2. Background**\n",
    "\n",
    "    2.1 Chain Rule\n",
    "    2.2 Computational Graph Structure\n",
    "    2.3 Dual Numbers\n",
    "    2.4 Elementary Functions\n",
    "    \n",
    "**3. Package Usage**\n",
    "\n",
    "    3.1 User Interaction\n",
    "    3.2 Getting Started via Virtual Environments\n",
    "    3.3 Importing AD20\n",
    "    3.4 Instantiating AD20 Objects\n",
    "    3.5 Newton's Method for a Scalar Valued Function\n",
    "    3.6 Future Extensions of the Basic Scalar Implementation\n",
    "    \n",
    "**4. Software Organization**\n",
    "\n",
    "    4.1 Directory Structure\n",
    "    4.2 Modules and Functionality\n",
    "    4.3 Testing and Coverage\n",
    "    4.4 Package Distribution\n",
    "    \n",
    "**5. Implementation**\n",
    "\n",
    "    5.1 Core Data Structures\n",
    "    5.2 Implemented Classes, Methods, and Attributes\n",
    "    5.3 External Dependencies\n",
    "    5.4 Elementary Functions\n",
    "    5.5 Future Implementation\n",
    "    \n",
    "**6. Project Extension**\n",
    "\n",
    "    6.1 Computational Graphs and Tables\n",
    "    6.2 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "The AD20 package performs the forward mode of automatic differentiation of user defined functions, evaluating both the function and its derivatives to machine precision.\n",
    "\n",
    "## 1.1 Automatic Differentiation as a Solution to the Problem of Computing Derivatives\n",
    "\n",
    "Differentiation is a fundamental operation for computational science. Used in a variety of applications from optimization to sensitivity analysis, differentiation is most useful when two conditions are met: it must be exact (up to machine precision) and computationally efficient.\n",
    "\n",
    "Automatic differentiation (AD) (i.e. algorithmic differentiation, computational differentiation) computes the derivative of a function, unique for its ability to handle complex combinations of functions without sacrificing the accuracy. Regardless of how complex the function may be, AD takes advantage of the fact that the function can be decomposed into a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). \n",
    "\n",
    "Through computing the derivatives of these basic elementary functions and repeatedly applying the chain rule, AD meets the two aforementioned conditions and distinguishes itself from other modes of differentiation, namely numerical differentiation and symbolic differentiation. \n",
    "\n",
    "**Numerical Differentiation through Finite Difference Methods:** \n",
    "This class of techniques uses the definition of a derivative,\n",
    "$$\\frac{df(x)}{dt} = \\lim_{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}$$\n",
    "to approximate the derivative by evaluating the right hand side for small $h$.  Such a technique is easy to code because it requires only defining and evaluating $f$, but it has limitations in precision due to truncation and roundoff errors, alongside the challenge of choosing an appropriately sized $h$.  While more complex finite difference schemes have been used to increase accuracy, all numerical differentiation remains only approximate with sensitivity to the choice in step size.\n",
    "\n",
    "**Symbolic Differentiation:**\n",
    "Symbolic differentiation addresses the shortcomings of approximation in numerical differentiation by computing derivatives to machine precision using expression trees, which quickly become inefficient to compute. \n",
    "\n",
    "Both of these options have been used in a variety of different applications to compute derivatives, but both have shortcomings that are addressed by automatic differentiation.  For this reason, for many computational applications, automatic differentiation is preferred:\n",
    "\n",
    "- While numerical differentiation may be easy to implement and can flexibly handle any type of function, accuracy is sacrificed due to truncation and rounding errors - numerical differentiation serves more as an estimation technique based on small inputs. Unlike numerical differentiation, automatic differentiation does not rely on approximating the derivative through the choice of a small perturbation in the input, and instead computes derivatives exactly to machine precision, thus avoiding these accuracy and stability problems.\n",
    "\n",
    "\n",
    "- While symbolic differentiation may ensure accuracy up to machine precision, computational efficiency is sacrified due to the nature of building complex expression trees. For complex functions, these expression trees can quickly become very large with many mathematical expressions. Unlike symbolic differentiation, automatic differentiation views functions as compositions of basic operations, remains accurate up to machine precision, and maintains computational efficiency since it does not require the buildup and evaluation of complex expression trees.\n",
    " \n",
    "Thus, it is clear that automatic differentiation has advantages over other commonly used techniques for computing derivatives. These advantages make the use of AD attractive to many scientific applications. \n",
    "\n",
    "## 1.2 Application of AD Techniques\n",
    "\n",
    "Through its improved accuracy and efficiency, AD has many different applications where accuracy, precision, and efficiency is crucial in computation. Some potential applications include: \n",
    "\n",
    "- Machine learning (ability to understand data and make models/predictions), where backpropagation is used to parameterize neural nets among other parameter optimization techniques\n",
    "- Parameter optimization (ability to choose best parameter values under given conditions), where methods requiring derivatives may be used to find the optima\n",
    "- Sensitivity analysis (ability to understand different factors and their impact), which requires computing partial derivatives with respect to different inputs and parameters\n",
    "- Physical modeling (ability to visualize and depict data through models), where different physical properties are related through derivatives (for example, acceleration is the derivative of velocity)\n",
    "- Probabilistic inference, where many sampling methods (for example, Hamiltonian Monte Carlo) are derivative based\n",
    "\n",
    "This large range of applications motivates the development of a package that can easily be used to compute derivatives up to machine precision efficiently, precisely the problem solved by automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background\n",
    "\n",
    "Here we discuss the mathematical concepts and computational process needed to perform automatic differentiation.\n",
    "\n",
    "## 2.1 The Chain Rule\n",
    "\n",
    "The chain rule forms the core concept behind automatic differentiation.  The chain rule gives the formula for calculating derivates of a composition of functions by sequentially taking derivatives of the outermost to innermost function.  The chain rule gives us that for a function $f\\left(g\\left(t\\right)\\right)$, we find the derivative of $f$ with respect to $t$ by multiplying a series of derivatives,\n",
    "$$\\dfrac{\\partial f}{\\partial t} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial t}.$$\n",
    "\n",
    "Similarly, AD performs differentiation by decomposing complex functions into combinations of simpler, more elementary functions then computing the derivatives of the elementary functions to piece them together to get the overall derivative. By expressing the function as a composition of elementary functions and operations, the derivative of the function can be calculated by applying the chain rule.\n",
    "\n",
    "The chain rule can be generalized to multiple dimensions, just as automatic differentiation can compute gradients of functions of multiple inputs.  For example, for the function $f(g(u, v), h(u, v))$, we have\n",
    "$$\\dfrac{\\partial f}{\\partial u} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial u} + \\dfrac{\\partial f}{\\partial h}\\dfrac{\\partial h}{\\partial u}$$\n",
    "and \n",
    "$$\\dfrac{\\partial f}{\\partial v} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial v} + \\dfrac{\\partial f}{\\partial h}\\dfrac{\\partial h}{\\partial v}$$\n",
    "where the gradient is given by\n",
    "$$\\nabla f = \\left(\\dfrac{\\partial f}{\\partial u}, \\dfrac{\\partial f}{\\partial v}\\right)$$\n",
    "\n",
    "In the most general form, we have for $f(g(x))$ with $x \\in \\mathbb{R}^m$ and $g \\in \\mathbb{R}^m$,\n",
    "\n",
    "$$ \\nabla_x f = \\sum_{i=1}^n \\frac{\\partial f}{\\partial g{i}}\\nabla_x g_i $$\n",
    "\n",
    "## 2.2 Graph Structure of Calculations\n",
    "\n",
    "Automatic differentation computes derivatives by constructing a function through a sequence of elementary operations and functions.  We can visualize the process of constructing a function in this way through a computational graph, where the nodes are functions and the directed edges describe the computations which connect the nodes.  By tracing a path from the input nodes to the output final function, we trace the same computations which are done by the automatic differentiation method to evaluate the function and its derivative.\n",
    "\n",
    "### Example: The Computational Graph\n",
    "Consider the example function $$f\\left(x,y\\right) = x^{3} + \\sin(5y)$$\n",
    "evaluated at $(x,y) = (1, \\frac{\\pi}{5})$.\n",
    "\n",
    " The evaluation trace looks like:\n",
    "\n",
    "| Trace | Elementary Function | Numerical Value |  Elementary Function Derivative | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  | \n",
    "| :---: | :-----------------: |:-------:| :-----------: | :-------------: | :----------------------: | :---------------------: | \n",
    "| $x_{1}$ | $x$ | $1$ | $\\dot{x_{1}}$|$1$|$0$|\n",
    "| $x_{2}$ | $y$ | $\\frac{\\pi}{5}$ | $\\dot{x_{2}}$|$0$|$1$|\n",
    "| $x_{3}$ | $x_{1}^3$ |1 |$3x_{1}^2\\dot{x_{1}}$|$3$|$0$|\n",
    "| $x_{4}$ | $5x_{2}$ | $\\pi$ | $5\\dot{x_{2}}$|$0$|$5$|\n",
    "| $x_{5}$ | $\\sin{x_{4}}$ |$\\sin(\\pi) = 0$ | $\\cos(x_{4})\\dot{x_{4}}$|$0$|$5\\cos(\\pi) = 5$|\n",
    "| $x_{6}$ | $x_{3}+x_{5}$ |$1$| $\\dot{x_{3}}+\\dot{x_{5}}$ |$3$|$5$|\n",
    "\n",
    "The evaluation trace can be visualized with a computational graph.\n",
    "\n",
    "![comp-graph](figs/compgraphppt.jpg)\n",
    "\n",
    "Through the same fundamental process, building complex functions through the composition of elementary functions, automatic differentiation computes derivatives exactly to machine precision.  Our package exploits different data structures, classes, and methods to implement this technique.\n",
    "\n",
    "## 2.3 Dual Numbers\n",
    "Analogously to complex numbers, a dual number has a real part and a dual part.  We write $$f = y + \\epsilon y^{\\prime},$$ where $y$ is the real part and $y^{\\prime}$ is the dual part.  The number $\\epsilon$ is defined as a special constant such that $\\epsilon^{2} = 0$.\n",
    "\n",
    "#### Properties of Dual Numbers\n",
    "Again by analogy to complex numbers, we can define several properties of the dual numbers, which are useful in performing calculations with them:\n",
    "* Conjugate:  $f^{*} = y - \\epsilon y^{\\prime}$.\n",
    "* Magnitude: $\\left|f\\right|^{2} = ff^{*} = \\left(y+\\epsilon y^{\\prime}\\right)\\left(y-\\epsilon y^{\\prime}\\right) = y^{2}$.\n",
    "* Polar form: $f = y\\left(1 + \\dfrac{y^{\\prime}}{y}\\right)$.\n",
    "\n",
    "More significantly, the dual numbers have the property that they can be used to compute derivatives of functions.  If we wish to compute the derivative of $f(x)$, the real part of $f(x+x'\\epsilon)$ gives us the value of the function while the dual part gives us the value of the derivative as can be seen in the following example from lecture,\n",
    "\n",
    "#### Example\n",
    "Let $f(z)=z^{2}$.  We know that the derivative is $f^{\\prime}(z) = 2zz^{\\prime} = 2z$.\n",
    "\n",
    "Consider instead evaluating, $f(z+z'\\epsilon)$, where we have extended z to the dual numbers, \n",
    "\n",
    "$$ f = \\left(z + \\epsilon z^{\\prime}\\right)^{2}$$\n",
    "$$= z^{2} + 2zz'\\epsilon + z'^{2}\\epsilon^{2} $$\n",
    "Since $\\epsilon^2 = 0$, \n",
    "$$ = z^{2} + 2zz^{\\prime}\\epsilon $$\n",
    "$$ = f(z) + f'(z)\\epsilon $$\n",
    "\n",
    "\n",
    "where we see that the real part is just the original function and the dual part is the derivative.\n",
    "\n",
    "While our implementation of automatic differentiation does not use dual numbers explicitly, it is inspired by this structure.  Each object that we evaluate will contain an attribute of its value, corresponding to the real part of a dual number, and an attribute for its derivative, corresponding to the dual part of a dual number.\n",
    "\n",
    "\n",
    "## 2.4 Elementary Functions\n",
    "Any complex equation can be broken into combinations of elementary operations and functions. The elementary operations include addition, subtraction, multiplication, division, and exponentiation.  The elementary functions include the natural exponential, the natural logarithm, and the trigonometric functions.  These functions are considered elementary because they form the core set of elements used to build more complex functions.\n",
    "\n",
    "We know how to explicitly compute the derivatives of both elementary operations and functions, given by the standard set of calculus rules.  We will not go into details about how to calculate the derivatives of those functions here, but more information can be found on the following link.\n",
    "\n",
    "http://www.nabla.hr/FU-DerivativeA5.htm\n",
    "\n",
    "Because every function is composed of elementary functions and we can take the derivative of a composition of functions by applying the chain rule, we can take the derivative of any function by knowing the derivative of these elementary functions.  It is this principle that forms the core of the automatic differentiation algorithm, implemented in this package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Package Usage\n",
    "\n",
    "## 3.1 User Interaction\n",
    "Users should use `ADnum` objects to represent mathematical objects for which they would like to evaluate a value or a derivative.  By forming `ADnum` objects for the function inputs, the elementary operations and functions defined for the `ADnum` class can be composed to create any desired function, which will also be of the `ADnum` class, with associated value and derivative attributes. All operations are defined for an `ADnum` object. Users need to create an `ADnum` object for each input variable and use all the mathematical functions defined in the `ADmath` library to implement special functions.\n",
    "\n",
    "## 3.2 Getting Started via Virtual Environments\n",
    "For now, use the following sequence to access the module.\n",
    "\n",
    "    git clone https://github.com/CS207-AD20/cs207-FinalProject.git\n",
    "\n",
    "You now have downloaded our git repository of the AD20 project. In the future, our package will be deployed through pip and can be accessed using the following command:\n",
    "\n",
    "    pip install AD20\n",
    "\n",
    "For deployment purposes, we recommend using a virtual environment in case you may have different packages with different dependencies. A virtual environment will make deployment easier by keeping multiple environments separate. \n",
    "\n",
    "Download virtualenv if you do not already have it installed:\n",
    "\n",
    "    sudo easy_install virtualenv\n",
    "\n",
    "At the top of your directory, initialize a new virtual environment called `env`, activate the virtual environment, and install all necessary packages:\n",
    "\n",
    "    virtualenv env\n",
    "    source env/bin/activate\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "`requirements.txt` has been created to help you install all the packages that you may need. The last command `pip install -r requirements.txt` will install those packages for you automatically.\n",
    "\n",
    "To deactivate the virtual environment once finished, deactivate it.\n",
    "\n",
    "    deactivate\n",
    "    \n",
    "\n",
    "## 3.3 Importing AD20\n",
    "In order to import and use `ADnum`, the user can use \n",
    "    \n",
    "    import AD20\n",
    "    \n",
    "For ease of use, and to follow the examples below, we suggest: \n",
    "\n",
    "\tfrom AD20.ADnum import ADnum\n",
    "    \n",
    "\tfrom AD20 import ADmath\n",
    "    \n",
    "In the future, we also plan to create an ADgraph module:\n",
    "\n",
    "    from AD20 import ADgraph\n",
    "\n",
    "\n",
    "## 3.4 Instantiating AD20\n",
    "After importing `AD20`, a user creates a class instance of an `ADnum` with some value to be used as input to a function. In the future, `AD20` will be able to handle scalar or vector functions with scalar or vector inputs.  Our current implementation performs automatic differentiation of scalar functions of a scalar variable.\n",
    "\n",
    "### Scalar Functions of Scalar Variables\n",
    "Sequence is as follows\n",
    "    1. initialize a variable (i.e. `x`) with a specific value that it will be evaluated on\n",
    "    2. create a function (i.e. `f`) with the variable and any other elementary functions from the ADmath module\n",
    "    3. `f.val` will return the value of the function evaluated at the specific value\n",
    "    4. `f.der` will return the derivative at the specific value\n",
    "\n",
    "```python\n",
    "    # simple scalar function, single variable\n",
    "    >>> x = ADnum(2)\n",
    "    >>> f = 3 * x**2\n",
    "    >>> print(f.val) \n",
    "    >>> print(f.der)\n",
    "    >>> print(x.val)\n",
    "    >>> print(x.der)\n",
    "    12\n",
    "    12\n",
    "    2\n",
    "    1\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "    # scalar function using elementary functions from ADmath\n",
    "    >>> import numpy as np\n",
    "    >>> x = ADnum(np.pi)\n",
    "    >>> f = ADmath.sin(x)\n",
    "    >>> print(f.val)\n",
    "    >>> print(f.der)\n",
    "    >>> print(x.val)\n",
    "    >>> print(x.der)\n",
    "    1.2246467991473532e-16 # ~0\n",
    "    -1.0\n",
    "    3.141592653589793\n",
    "    1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Example Usage Including Imports for a Scalar Function of a Scalar Variable\n",
    "\n",
    "Here we show several examples of scalar functions of scalar variables, including an example of using our package to implement Newton's Method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiwhanyoon/Desktop/cs207/cs207-FinalProject\n",
      "/Users/jiwhanyoon/Desktop/cs207/cs207-FinalProject/AD20\n"
     ]
    }
   ],
   "source": [
    "#commands to change to the \n",
    "%pwd \n",
    "%cd ..\n",
    "%cd AD20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "[0. 2.]\n",
      "3.0\n",
      "[1. 0.]\n",
      "4.0\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#necessary imports\n",
    "import AD20\n",
    "import numpy as np\n",
    "from AD20.ADnum_multivar import ADnum\n",
    "from AD20 import ADmath\n",
    "\n",
    "x = ADnum(3, ins = 2, ind = 0) # Step 1: intialize x to a specific value\n",
    "y = ADnum(4, ins = 2, ind= 1)\n",
    "f = 2 * y\n",
    "# f = 2 * x # Step 2: write a function which we would like to take the derivative\n",
    "\n",
    "# Steps 3 and 4: Use the class attributes to access the value and deriviative of the function at the value of the input x \n",
    "\n",
    "print(f.val) #should equal 81\n",
    "print(f.der) #should equal 72\n",
    "print(x.val) #should equal 3\n",
    "print(x.der) #should equal 1\n",
    "print(y.val)\n",
    "print(y.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value and derivative of ADnum object must be numeric.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/cs207/cs207-FinalProject/AD20/AD20/ADnum_multivar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ins'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ins'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/cs207/cs207-FinalProject/AD20/AD20/ADnum_multivar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must provide ins and ind if der not provided.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Must provide ins and ind if der not provided.'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-57ff931ff6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#another example with a trignometric function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mADnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Step 1: initialize x, this time at pi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mADmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Step 2: create a function, using elementary functions from the ADmath module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Steps 3 and 4: Use the class attributes to access the value and derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cs207/cs207-FinalProject/AD20/AD20/ADnum_multivar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of derivative does not match number of inputs.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Value and derivative of ADnum object must be numeric.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Value and derivative of ADnum object must be numeric."
     ]
    }
   ],
   "source": [
    "#another example with a trignometric function\n",
    "x = ADnum(np.pi) # Step 1: initialize x, this time at pi\n",
    "f = ADmath.sin(x) # Step 2: create a function, using elementary functions from the ADmath module\n",
    "\n",
    "#Steps 3 and 4: Use the class attributes to access the value and derivative\n",
    "print(f.val) # should print 1.22e-16 due to floating point error in numpy implementation (should be 0)\n",
    "print(f.der) # should print -1.0\n",
    "print(x.val) # should print 3.14\n",
    "print(x.der) # should print 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to easily be able to access the value and derivative of a function at many different points.  As an alternative to the method for defining `f` in the previous two examples, we could define `f` as a python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example to easily access value and derivative at multiple points by defining f as a function\n",
    "def f(x):\n",
    "    return x+ADmath.exp(x)\n",
    "\n",
    "#get the value and derivative at 0\n",
    "y = ADnum(0)\n",
    "print(f(y).val, f(y).der)\n",
    "\n",
    "#an alternate approach to get the value and derivative at 1\n",
    "print(f(ADnum(1)).val, f(ADnum(1)).der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the above example, we required the natural exponential, an elementary function, to be used from the ADmath package, so that f may take as input and return an ADnum object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Newton's Method for a Scalar Valued Function\n",
    "One basic application of differentiation is Newton's method for finding roots of a function.  For demonstration of using our package for such an application, we will consider the function\n",
    "$$f(x) = x^2 + \\sin(x)$$\n",
    "which we know has a root at $x=0$.  The plot below also shows that the function has an additional root near -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-2.5, 2.55, 1000)\n",
    "f = x**2+np.sin(x)\n",
    "\n",
    "plt.plot(x, f, linewidth = 2)\n",
    "plt.plot(x, np.zeros((1000,)), '--')\n",
    "plt.xlabel('x', fontsize = 16)\n",
    "plt.ylabel('f(x)', fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize =14)\n",
    "plt.title('Plot of f(x) Showing Two Roots', fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of Newton's method using AD20, without hardcoding the derivative\n",
    "\n",
    "#function that we wish to find the roots of\n",
    "def f(x):\n",
    "    return x**2+ADmath.sin(x)\n",
    "\n",
    "#Newton's method\n",
    "x = ADnum(1) #set an initial guess for the root\n",
    "\n",
    "for i in range(1000):\n",
    "    dx = -f(x).val/f(x).der #get change using ADnum attributes\n",
    "    if np.abs(dx) < .000001: #check if within some tolerance\n",
    "        print('Root found at:' + str(x.val))\n",
    "        break\n",
    "    x = x+dx #update the guess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we found the root at zero.  Using a different initialization point, we can find the other root of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ADnum(-1) #set an initial guess for the root\n",
    "\n",
    "for i in range(1000):\n",
    "    dy = -f(y).val/f(y).der #get change using ADnum attributes\n",
    "    if np.abs(dy) < .000001: #check if within some tolerance\n",
    "        print('Root found at:' + str(y.val))\n",
    "        break\n",
    "    y = y+dy #update the guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Future Extensions of the Basic Scalar Implementation\n",
    "In the future, we will extend our basic package to find the gradient of scalar valued functions of multiple variables and the Jacobian of vector valued functions of vector valued input.  The following gives an outline of the usage of this future work.\n",
    "\n",
    "\n",
    "### Functions of Multiple Variables\n",
    "In case case with a function with more than one variable, the sequence is similar except when creating ADnum objects, the user must specify the total number of input variables, and the index of each variable in the gradient (so the the constructor of the ADnum class can correctly assign the derivative of the input variable):\n",
    "    1. initialize each variable to a specific value where the function should be evaluated\n",
    "    2. return the gradient as a numpy array when `f.der` is called \n",
    "    \n",
    "```python\n",
    "    # scalar function, multi variable\n",
    "    >>> x = ADnum(2, ins = 2, ind = 0)\n",
    "    >>> y = ADnum(3, ins = 2, ind = 1)\n",
    "    >>> f = 3 * x**3 + 2 * y**3\n",
    "    >>> print(f.val)\n",
    "    >>> print(f.der)\n",
    "    >>> print(x.val)\n",
    "    >>> print(x.der)\n",
    "    >>> print(y.val)\n",
    "    >>> print(y.der)\n",
    "    78\n",
    "    np.array([36, 54])\n",
    "    2\n",
    "    np.array([1, 0])\n",
    "    3\n",
    "    np.array([0, 1])\n",
    "```\n",
    "\n",
    "### Vector-valued Functions\n",
    "Each component of a vector valued function is just a scalar valued function of one or more input variables.  Thus, we can easily combine the previous results to get the Jacobian of a vector valued function.  By updating our methods to broadcast appropriately for an array, we can easily access these attributes,\n",
    "\n",
    "```python\n",
    "    >>> x = ADnum(2, ins = 2, ind = 0)\n",
    "    >>> y = ADnum(3, ins = 2, ind = 0)\n",
    "    >>> F = [x**2, x+y, 4*y]\n",
    "    >>> F[1].der = [1 , 1]\n",
    "    >>> F.der = [[4, 0], [1, 1], [0, 4]]\n",
    "    >>> F[0].val = 4\n",
    "    >>> F.val = [4, 5, 12]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Software Organization\n",
    "We would like to let the user use all numerical operations defined in our AD20 package. The AD20 package contains the `ADnum` module, the `ADmath` module, and the `ADgraph` module.\n",
    "\n",
    "For either a scalar or vector input (either as a numpy array or a list), we will convert the input into an `ADnum` object, which can interact with the other modules. `ADnum` will also contain an overloaded version of basic operations, including addition, subtraction, multiplication, division, and exponentiation, so that the value and derivative are correctly updated after combining ADnum objects through each of these operations.\n",
    "\n",
    "For special functions, we will use `ADmath` to compute the numerical values and the corresponding derivatives. In particular, `ADmath` will contain functions abs, exp, log, sin, cos, and tan.\n",
    "\n",
    "To show a calculation graph, we use `ADgrap`h (and `ADtable`) to show the forward mode calculation process.\n",
    "\n",
    "###  4.1 Directory Structure\n",
    "    AD20/\n",
    "        AD20/\n",
    "            __init__.py\n",
    "            ADnum.py\n",
    "            ADmath.py\n",
    "\n",
    "        Tests/\n",
    "            __init__.py\n",
    "            test_AD20.py\n",
    "    docs/\n",
    "        Milestone 1.ipynb\n",
    "        Milestone 2.ipynb\n",
    "        figs/\n",
    "    README.md\n",
    "    setup.cfg\n",
    "    requirements.txt\n",
    "    LICENSE\n",
    "\n",
    "###  4.2 Modules and Functionality\n",
    "Our package consists of three main modules:\n",
    "\n",
    "- **ADnum:** Contains the `ADnum` class (fully described below).  Create `ADnum` objects, which (inspired by the dual numbers) are defined by the attributes of a value and a derivative, from numbers or tuples.  Define all of the numerical operations for `ADnum` objects, so that they correctly track all derivatives.\n",
    "\n",
    "- **ADmath:** Define elementary functions for `ADnum` objects, correctly tracking all of the derivatives.\n",
    "\n",
    "and in future implementation,\n",
    "\n",
    "- **ADgraph:** Create `ADgraph` objects, which can be used to show the computation process in either a graph (ADgraph.py) or table (ADtable.py)\n",
    "\n",
    "###  4.3 Testing and Coverage\n",
    "All tests are contained in the test_AD20.py file in the tests directory (see the repo structure above).  We will use pytest to perform our testing, using `TravisCI` and `Coveralls` for continuous integration and verifying code coverage respectively.  The test suite contains unit tests for all of the class methods implemented in ADnum and all the elementary functions implemented in ADmath.  This suite also contains several functions which are composed of several different operations and elementary functions for more advanced testing.\n",
    "\n",
    "###  4.4 Package Distribution\n",
    "For the final project submission, we will use `PIP` in `PyPi` to distribute our package. This will allow the user to install the package by using the command\n",
    "\n",
    "    pip install AD20\n",
    "    \n",
    "Note that the current method for installing `AD20` through git is outlined in user interaction with numpy as the only current external dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementation\n",
    "Automatic differentiation is implemented through the use of `ADnum` objects and building the functions for which we want to take derivatives from these `ADnum` objects as well as the special elementary functions defined for `ADnum` objects in the `ADmath` module.  Each of these functions is itself an `ADnum` object so has an associated value and derivative which was updated when constructing the `ADnum` object through basic operations and elementary functions.\n",
    "\n",
    "### 5.1 Core Data Structures\n",
    "`ADnum` objects can be thought of as a tuple, where the first entry is the value and the second entry is the derivative.  Each of these attributes is either a scalar or a numpy array for ease of computation.  In the case of scalar input, the derivative is also a scalar.  For vector valued input, the derivative is the gradient of the function, stored as a numpy array.\n",
    "\n",
    "In order to build and store computational graphs in the ADgraph module, we will use a dictionary to represent the graph, where the keys are the nodes of the graph, stored as `ADnum` objects, and the values associated with each key are the children of that node, stored as lists of ADnum objects.\n",
    "\n",
    "### 5.2 Implemented Classes, Methods, and Attributes\n",
    "The main class is the `ADnum` module, which is used to create `ADnum` objects.  It takes as input a single scalar input or a vector input (as a numpy array) and outputs an `ADnum` object.  The `ADnum` objects store the current value of the function and its derivative as attributes. \n",
    "\n",
    "These two attributes represent the two major functionalities desired of the class.  The `val` attribute is the ADnum object evaluated at the given value and the `der` attribute is its derivative at the given value. The constructor for this class, sets the value of the object and optionally also sets the value of its derivative,\n",
    "\n",
    "```python\n",
    "#ADnum.py\n",
    "class ADnum():\n",
    "    def __init__(self, a, d = 1):\n",
    "        self.val = a\n",
    "        self.der = d\n",
    "        self.graph = {} #for future implementation\n",
    "```\n",
    "\n",
    "The `ADnum` class also includes methods to overload basic operations, __add__(), __radd__(), __mul__(), __rmul__(), __sub__(), __rsub__(), __truediv__(), __rtruediv__(), __pow__(), and __rpow__().  The result of overloading is that the adding, subtracting, multiplying, dividing, or exponentiating two `ADnum` objects returns an `ADnum` object as well as addition or multiplication by a constant.  For example, Y1, Y2, and Y3 are all recognized as `ADnum` objects:\n",
    "\n",
    "```python\n",
    "    X1= ADnum(7)\n",
    "    X2 = ADnum(15)\n",
    "    Y1 = X1+X2\n",
    "    Y2 = X1*X2+X1\n",
    "    Y3 = 5*X1+X2+100\n",
    "```\n",
    "\n",
    "The resulting ADnum objects have both a value and derivative.  An example overloaded function is the following:\n",
    "\n",
    "\n",
    "```python\n",
    "#ADnum.py\n",
    "def __mul__(self,other):\n",
    "        try:\n",
    "            return ADnum(self.val*other.val, self.val*other.der+self.der*other.val)\n",
    "        except AttributeError:\n",
    "            other = ADnum(other, 0)\n",
    "            return self*other\n",
    "```\n",
    "\n",
    "By combining simple `ADnum` objects with basic operations and simple functions, we can construct any function we like.\n",
    "\n",
    "```python\n",
    "    X = ADnum(4)\n",
    "    F = X + ADmath.sin(4-x)\n",
    "```    \n",
    "Where F is now an `ADnum` object, and ADmath.sin() is a specially defined sine function which takes as input an `ADnum` object and returns an `ADnum` object, which allows us to evaluate F and its derivative,\n",
    "\n",
    "```python\n",
    "    F.val = 4\n",
    "    F.der = 0\n",
    "    X.val = 4\n",
    "    X.der = 1\n",
    "```\n",
    "\n",
    "In addition to the sine function used in the example above, the `ADmath` module also implements the trigonometric functions: `sin()`, `cos()`, `tan()`, `csc()`, `sec()`, `cot()`, the inverse trigonometric functions: `arcsin()`, `arccos()`, `arctan()`, the hyperbolic trig functions: `sinh()`, `cosh()`, `tanh()`, and the natural exponential `exp()` and natural logarithm `log()`.  All of the functions defined in the `ADmath` module define elementary functions of `ADnum` objects, so that the output is also an `ADnum` object with the val and deriv attributes updated appropriately.  For example,\n",
    "\n",
    "```python\n",
    "#ADmath.py\n",
    "def sin(X):\n",
    "    try:\n",
    "        return adn.ADnum(np.sin(X.val), np.cos(X.val)*X.der)\n",
    "    except AttributeError:\n",
    "        X = adn.ADnum(X, 0)\n",
    "        return sin(X)\n",
    "    \n",
    "def log(X):\n",
    "    try:\n",
    "        return adn.ADnum(np.log(X.val), 1/X.val*X.der)\n",
    "    except AttributeError:\n",
    "        X = adn.ADnum(X, 0)\n",
    "        return log(X)\n",
    "```\n",
    "\n",
    "We will also implement a class, `ADgraph`, for computational graphs.  The constructor takes as input a dictionary, as described above where the keys are nodes and values are the children of the key node.  The `ADgraph` class will be constructed from a dictionary, stored in the attribute dict.  This class will also have an attribute inputs, which stores the nodes which have no parents.  This class will implement methods to display the computational graphs and tables used to compute the derivatives of the `ADnum` objects.\n",
    "\n",
    "### 5.3 External Dependencies\n",
    "In order to implement the elementary functions, our ADmath relies on numpy’s implementation of the trigonometric functions, exponential functions, and natural logarithms for evaluation of these special functions, as demonstrated in the definition of the sine function for `ADnum` objects above.\n",
    "\n",
    "We will also use numpy to implement matrix and vector multiplication in cases where the function is either vector valued or takes a vector as an input.\n",
    "\n",
    "### 5.4 Elementary Functions\n",
    "As outlined above, all elementary operations are defined for `ADnum` objects within the `ADnum` class and we have a special `ADmath` module which defines the trigonometric, exponential, and logarithmic functions to be used on ADnum objects, so that they both take as input and return an `ADnum` object, completing the set of defintions of all elementary operations and functions that can be composed to construct more complex functions.\n",
    "\n",
    "### 5.5 Future Implementation\n",
    "Our current implementation performs automatic differentiation for scalar functions of scalar variables.  Our constructor needs to be modified for functions of multiple inputs so that the `der` attribute will now be represented by a numpy array in these cases.  Minor revisions will also need to be applied to the methods defined in the `ADnum` and `ADmath` classes to ensure that these multidimensional `der` attributes are correctly updated, using the correct elementwise matrix multiplication for the numpy arrays.\n",
    "\n",
    "We will also need to implement the functionality described in section 6, which includes an implementation of reverse mode for backpropagation and an implementation of the ADgraph class for building computational graphs and tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Project Extension\n",
    "\n",
    "In order to expand our project from the basic forward mode automatic differentiation, we will make two additional developments for pedagogical and application purposes.\n",
    "\n",
    "### 6.1 Computational Graphs and Tables\n",
    "We will implement the class `ADgraph` which stores the computational graph used to compute the derivatives in forward mode.  For every operation we create an additional node which represents another trace in the program.  This graph will be stored as an attribute of the `ADnum` objects.  For pedagogical purposes, we want to be able to visualize this process so we will use external packages for displaying graphs where the edge labels display the corresponding operation.  Correspondingly, we will also develop the functionality to display a table showing the trace, elementary operation, value, and derivative at each step.  Such a tool could be useful in the classroom for teaching students how automatic differentiation works.\n",
    "\n",
    "This will require modifying all of our methods to correctly add to the dictionary which contains the computational graph information for each operation that we have previously overloaded.  This will also involve the added challenge of ensuring compatability between our program and an external program for visualizing graphs and tables.\n",
    "\n",
    "### 6.2 Backpropagation\n",
    "The computational graph is also necessary to implement the reverse mode of automatic differentiation, which is important to many applications.  For example, backpropagation is the underlying method to fit the parameters of neural networks.  We will use the computational graph to store the structure needed to backpropagate derivatives.  Thus, we will have expanded our project to perform both forward and reverse automatic differentiation, making the package more suitable to a variety of applications. \n",
    "\n",
    "To perform backpropagation requires writing methods to correctly traverse the computational graph and propagate derivatives from the output through the intermediates to the inputs, which will require developing new methods for the `ADgraph`class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
