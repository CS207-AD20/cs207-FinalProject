{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AD20 Milestone 2\n",
    "Group 20: Lindsey Brown, Xinyue Wang, Kevin Yoon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "**1. Introduction**\n",
    "\n",
    "    1.1 Automatic Differentiation as a Solution to the Problem of Computing Derivatives\n",
    "    1.2 Application of AD Techniques\n",
    "    \n",
    "**2. Background**\n",
    "\n",
    "    2.1 Chain Rule\n",
    "    2.2 Computational Graph Structure\n",
    "    2.3 Dual Numbers\n",
    "    2.4 Elementary Functions\n",
    "    \n",
    "**3. Package Usage**\n",
    "\n",
    "    3.1 User Interaction\n",
    "    3.2 Getting Started via Virtual Environments\n",
    "    3.2 Importing AD20\n",
    "    3.3 Instantiating AD20 Objects\n",
    "    \n",
    "**4. Software Organization**\n",
    "\n",
    "    4.1 Directory Structure\n",
    "    4.2 Modules and Functionality\n",
    "    4.3 Testing and Coverage\n",
    "    4.4 Package Distribution\n",
    "    \n",
    "**5. Implementation**\n",
    "\n",
    "    5.1 Core Data Structures\n",
    "    5.2 Classes\n",
    "    5.3 Class Methods and Attributes\n",
    "    5.4 External Dependencies\n",
    "    5.5 Elementary Functions\n",
    "    5.6 What Comes Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "The AD20 package performs the forward mode of automatic differentiation of user defined functions, evaluating both the function and its derivatives to machine precision.\n",
    "\n",
    "## 1.1 Automatic Differentiation as a Solution to the Problem of Computing Derivatives\n",
    "\n",
    "Differentiation is a fundamental operation for computational science. Used in a variety of applications from optimization to sensitivity analysis, differentiation is most useful when two conditions are met: it must be exact (up to machine precision) and computationally efficient.\n",
    "\n",
    "Automatic differentiation (AD) (i.e. algorithmic differentiation, computational differentiation) computes the derivative of a function, unique for its ability to handle complex combinations of functions without sacrificing the accuracy. Regardless of how complex the function may be, AD takes advantage of the fact that the function can be decomposed into a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). \n",
    "\n",
    "Through computing the derivatives of these basic elementary functions and repeatedly applying the chain rule, AD meets the two aforementioned conditions and distinguishes itself from other modes of differentiation, namely numerical differentiation and symbolic differentiation. \n",
    "\n",
    "**Numerical Differentiation through Finite Difference Methods:** \n",
    "This class of techniques uses the definition of a derivative,\n",
    "$$\\frac{df(x)}{dt} = \\lim_{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}$$\n",
    "to approximate the derivative by evaluating the right hand side for small $h$.  Such a technique is easy to code because it requires only defining and evaluating $f$, but it has limitations in precision due to truncation and roundoff errors, alongside the challenge of choosing an appropriately sized $h$.  While more complex finite difference schemes have been used to increase accuracy, all numerical differentiation remains only approximate with sensitivity to the choice in step size.\n",
    "\n",
    "**Symbolic Differentiation:**\n",
    "Symbolic differentiation addresses the shortcomings of approximation in numerical differentiation by computing derivatives to machine precision using expression trees, which quickly become inefficient to compute. \n",
    "\n",
    "Both of these options have been used in a variety of different applications to compute derivatives, but both have shortcomings that are addressed by automatic differentiation.  For this reason, for many computational applications, automatic differentiation is preferred:\n",
    "\n",
    "- While numerical differentiation may be easy to implement and can flexibly handle any type of function, accuracy is sacrificed due to truncation and rounding errors - numerical differentiation serves more as an estimation technique based on small inputs. Unlike numerical differentiation, automatic differentiation does not rely on approximating the derivative through the choice of a small perturbation in the input, and instead computes derivatives exactly to machine precision, thus avoiding these accuracy and stability problems.\n",
    "\n",
    "\n",
    "- While symbolic differentiation may ensure accuracy up to machine precision, computational efficiency is sacrified due to the nature of building complex expression trees. For complex functions, these expression trees can quickly become very large with many mathematical expressions. Unlike symbolic differentiation, automatic differentiation views functions as compositions of basic operations, remains accurate up to machine precision, and maintains computational efficiency since it does not require the buildup and evaluation of complex expression trees.\n",
    " \n",
    "Thus, it is clear that automatic differentiation has advantages over other commonly used techniques for computing derivatives. These advantages make the use of AD attractive to many scientific applications. \n",
    "\n",
    "## 1.2 Application of AD Techniques\n",
    "\n",
    "Through its improved accuracy and efficiency, AD has many different applications where accuracy, precision, and efficiency is crucial in computation. Some potential applications include: \n",
    "\n",
    "- Machine learning (ability to understand data and make models/predictions), where backpropagation is used to parameterize neural nets among other parameter optimization techniques\n",
    "- Parameter optimization (ability to choose best parameter values under given conditions), where methods requiring derivatives may be used to find the optima\n",
    "- Sensitivity analysis (ability to understand different factors and their impact), which requires computing partial derivatives with respect to different inputs and parameters\n",
    "- Physical modeling (ability to visualize and depict data through models), where different physical properties are related through derivatives (for example, acceleration is the derivative of velocity)\n",
    "- Probabilistic inference, where many sampling methods (for example, Hamiltonian Monte Carlo) are derivative based\n",
    "\n",
    "This large range of applications motivates the development of a package that can easily be used to compute derivatives up to machine precision efficiently, precisely the problem solved by automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Background\n",
    "\n",
    "Here we discuss the mathematical concepts and computational process needed to perform automatic differentiation.\n",
    "\n",
    "## 2.1 The Chain Rule\n",
    "\n",
    "The chain rule forms the core concept behind automatic differentiation.  The chain rule gives the formula for calculating derivates of a composition of functions by sequentially taking derivatives of the outermost to innermost function.  The chain rule gives us that for a function $f\\left(g\\left(t\\right)\\right)$, we find the derivative of $f$ with respect to $t$ by multiplying a series of derivatives,\n",
    "$$\\dfrac{\\partial f}{\\partial t} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial t}.$$\n",
    "\n",
    "Similarly, AD performs differentiation by decomposing complex functions into combinations of simpler, more elementary functions then computing the derivatives of the elementary functions to piece them together to get the overall derivative. By expressing the function as a composition of elementary functions and operations, the derivative of the function can be calculated by applying the chain rule.\n",
    "\n",
    "The chain rule can be generalized to multiple dimensions, just as automatic differentiation can compute gradients of functions of multiple inputs.  For example, for the function $f(g(u, v), h(u, v))$, we have\n",
    "$$\\dfrac{\\partial f}{\\partial u} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial u} + \\dfrac{\\partial f}{\\partial h}\\dfrac{\\partial h}{\\partial u}$$\n",
    "and \n",
    "$$\\dfrac{\\partial f}{\\partial v} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial v} + \\dfrac{\\partial f}{\\partial h}\\dfrac{\\partial h}{\\partial v}$$\n",
    "where the gradient is given by\n",
    "$$\\nabla f = \\left(\\dfrac{\\partial f}{\\partial u}, \\dfrac{\\partial f}{\\partial v}\\right)$$\n",
    "\n",
    "In the most general form, we have for $f(g(x))$ with $x \\in \\mathbb{R}^m$ and $g \\in \\mathbb{R}^m$,\n",
    "\n",
    "$$ \\nabla_x f = \\sum_{i=1}^n \\frac{\\partial f}{\\partial g{i}}\\nabla_x g_i $$\n",
    "\n",
    "## 2.2 Graph Structure of Calculations\n",
    "\n",
    "Automatic differentation computes derivatives by constructing a function through a sequence of elementary operations and functions.  We can visualize the process of constructing a function in this way through a computational graph, where the nodes are functions and the directed edges describe the computations which connect the nodes.  By tracing a path from the input nodes to the output final function, we trace the same computations which are done by the automatic differentiation method to evaluate the function and its derivative.\n",
    "\n",
    "### Example: The Computational Graph\n",
    "Consider the example function $$f\\left(x,y\\right) = x^{3} + \\sin(5y)$$\n",
    "evaluated at $(x,y) = (1, \\frac{\\pi}{5})$.\n",
    "\n",
    " The evaluation trace looks like:\n",
    "\n",
    "| Trace | Elementary Function | Numerical Value |  Elementary Function Derivative | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  | \n",
    "| :---: | :-----------------: |:-------:| :-----------: | :-------------: | :----------------------: | :---------------------: | \n",
    "| $x_{1}$ | $x$ | $1$ | $\\dot{x_{1}}$|$1$|$0$|\n",
    "| $x_{2}$ | $y$ | $\\frac{\\pi}{5}$ | $\\dot{x_{2}}$|$0$|$1$|\n",
    "| $x_{3}$ | $x_{1}^3$ |1 |$3x_{1}^2\\dot{x_{1}}$|$3$|$0$|\n",
    "| $x_{4}$ | $5x_{2}$ | $\\pi$ | $5\\dot{x_{2}}$|$0$|$5$|\n",
    "| $x_{5}$ | $\\sin{x_{4}}$ |$\\sin(\\pi) = 0$ | $\\cos(x_{4})\\dot{x_{4}}$|$0$|$5\\cos(\\pi) = 5$|\n",
    "| $x_{6}$ | $x_{3}+x_{5}$ |$1$| $\\dot{x_{3}}+\\dot{x_{5}}$ |$3$|$5$|\n",
    "\n",
    "The evaluation trace can be visualized with a computational graph.\n",
    "\n",
    "![comp-graph](figs/compgraphppt.jpg)\n",
    "\n",
    "Through the same fundamental process, building complex functions through the composition of elementary functions, automatic differentiation computes derivatives exactly to machine precision.  Our package exploits different data structures, classes, and methods to implement this technique.\n",
    "\n",
    "## 2.3 Dual Numbers\n",
    "Analogously to complex numbers, a dual number has a real part and a dual part.  We write $$f = y + \\epsilon y^{\\prime}$$, where $y$ is the real part and $y^{\\prime}$ is the dual part.  The number $\\epsilon$ is defined as a special constant such that $\\epsilon^{2} = 0$.\n",
    "\n",
    "#### Properties of Dual Numbers\n",
    "Again by analogy to complex numbers, we can define several properties of the dual numbers, which are useful in performing calculations with them:\n",
    "* Conjugate:  $f^{*} = y - \\epsilon y^{\\prime}$.\n",
    "* Magnitude: $\\left|f\\right|^{2} = ff^{*} = \\left(y+\\epsilon y^{\\prime}\\right)\\left(y-\\epsilon y^{\\prime}\\right) = y^{2}$.\n",
    "* Polar form: $f = y\\left(1 + \\dfrac{y^{\\prime}}{y}\\right)$.\n",
    "\n",
    "More significantly, the dual numbers have the property that they can be used to compute derivatives of functions.  If we wish to compute the derivative of $f(x)$, the real part of $f(x+x'\\epsilon)$ gives us the value of the function while the dual part gives us the value of the derivative as can be seen in the following example from lecture,\n",
    "\n",
    "#### Example\n",
    "Let $f(z)=z^{2}$.  We know that the derivative is $f^{\\prime}(z) = 2zz^{\\prime} = 2z$.\n",
    "\n",
    "Consider instead evaluating, $f(z+z'\\epsilon)$, where we have extended z to the dual numbers, \n",
    "\n",
    "$$ f = \\left(z + \\epsilon z^{\\prime}\\right)^{2}$$\n",
    "$$= z^{2} + 2zz'\\epsilon + z'^{2}\\epsilon^{2} $$\n",
    "Since $\\epsilon^2 = 0$, \n",
    "$$ = z^{2} + 2zz^{\\prime}\\epsilon $$\n",
    "$$ = f(z) + f'(z)\\epsilon $$\n",
    "\n",
    "\n",
    "where we see that the real part is just the original function and the dual part is the derivative.\n",
    "\n",
    "While our implementation of automatic differentiation does not use dual numbers explicitly, it is inspired by this structure.  Each object that we evaluate will contain an attribute of its value, corresponding to the real part of a dual number, and an attribute for its derivative, corresponding to the dual part of a dual number.\n",
    "\n",
    "\n",
    "## 2.4 Elementary Functions\n",
    "Any complex equation can be broken into combinations of elementary operations and functions. The elementary operations include the addition, subtraction, multiplication, division, and exponentiation.  The elementary functions include the natural exponential, natural logarithm, and the trigonometric functions.  These functions are considered elementary because they form the core set of elements used to build more complex functions.\n",
    "\n",
    "We know how to explicitly compute the derivatives of both elementary operations and functions, given by the standard set of calculus rules.  We will not go into details about how to calculate the derivatives of those functions here, but more information can be found on the following link.\n",
    "\n",
    "http://www.nabla.hr/FU-DerivativeA5.htm\n",
    "\n",
    "Because every function is composed of elementary functions and we can take the derivative of a composition of functions by applying the chain rule, we can take the derivative of any function by knowing the derivative of these elementary functions.  It is this principle that forms the core of the automatic differentiation algorithm, implemented in this package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Package Usage\n",
    "\n",
    "## 3.1 User Interaction\n",
    "Users should use ADnum objects to represent mathematical objects for which they would like to evaluate a value or a derivative.  By forming ADnum objects for the function inputs, the elementary operations and functions defined for the ADnum class can be composed to create any desired function, which will also be of the ADnum class, with associated value and derivative attributes. All operations are defined for an ADnum object. Users need to create an ADnum object for each input variable and use all the mathematical functions defined in the ADmath library to implement special functions.\n",
    "\n",
    "## 3.2 Getting Started via Virtual Environments\n",
    "For now, use the following sequence to access the module.\n",
    "\n",
    "    git clone https://github.com/CS207-AD20/cs207-FinalProject.git\n",
    "\n",
    "You now have downloaded our git repository of the AD20 project. Once we get `pip` running, use the following command.\n",
    "\n",
    "    pip install AD20\n",
    "\n",
    "For deployment purposes, we recommend using a virtual environment in case you may have different packages with different dependencies. For a quick ramp up, a virtual environment will make deployment easier by keeping multiple environments separate. \n",
    "\n",
    "Download virtualenv if you don't have it.\n",
    "\n",
    "    sudo easy_install virtualenv\n",
    "\n",
    "At the top of your directory, initialize a new virtual environment called `env`, activate the virtual environment, and install all necessary packages\n",
    "\n",
    "    virtualenv env\n",
    "    source env/bin/activate\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "`requirements.txt` has been created to help you install all the packages that you may need. The last command `pip install -r requirements.txt` will install those packages for you automatically.\n",
    "\n",
    "To deactivate the virtual environment once finished, deactivate it.\n",
    "\n",
    "    deactivate\n",
    "    \n",
    "\n",
    "## 3.3 Importing AD20\n",
    "In order to import and use ADnum, the user can use \n",
    "    \n",
    "    import AD20\n",
    "    \n",
    "to import complete functionality of the package or \n",
    "\n",
    "\tfrom AD20 import ADnum\n",
    "    \n",
    "\tfrom AD20 import ADmath\n",
    "    \n",
    "\tfrom AD20 import ADgraph\n",
    "\n",
    "to use only specific modules.\n",
    "\n",
    "## 3.4 Instantiating AD20\n",
    "After importing AD20, a user creates a class instance of an ADnum with some value to be used as input to a more complex function. AD20 will be able to handle scalar or vector functions with scalar or vector inputs (i.e. one-dimensional vs two-dimensional).\n",
    "\n",
    "### Scalar Functions with Scalar Inputs\n",
    "Sequence is as follows\n",
    "    1. initialize a variable (i.e. `x`) with a specific value that it will be evaluated on\n",
    "    2. create a function (i.e. `f`) with the variable and any other elementary functions\n",
    "    3. `f.val` will return the value of the function evaluated at the specific value\n",
    "    4. `f.der(x)` will return the derivative\n",
    "\n",
    "```python\n",
    "    #simple scalar function, single variable\n",
    "    >>> x = ADnum(2)\n",
    "    >>> f = 3 * x**2\n",
    "    >>> print(f.val) \n",
    "    >>> print(f.der) \n",
    "    12\n",
    "    12\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "    #complex scalar function, single variable\n",
    "    >>> x = ADnum(1)\n",
    "    >>> f = 3 * x**3 + ADmath.sin(x)\n",
    "    >>> print(f.val)\n",
    "    >>> print(f.der)\n",
    "    0\n",
    "    1\n",
    "```\n",
    "\n",
    "In case case with a function with more than one variable, the sequence is similar except\n",
    "    1. calculate each gradient separately (`f.der(x`) & `f.der(y)`)\n",
    "    2. return the entire gradient as a list when `f.gradient` is called \n",
    "    \n",
    "```python\n",
    "    #complex scalar function, multi variable\n",
    "    >>> x = ADnum(2)\n",
    "    >>> y = ADnum(3)\n",
    "    >>> f = 3 * x**3 + 2 * y**3\n",
    "    >>> print(f.val)\n",
    "    >>> print(f.der(x))\n",
    "    >>> print(f.der(y))\n",
    "    >>> print(f.gradient)\n",
    "    78\n",
    "    36\n",
    "    54\n",
    "    [36, 54]\n",
    "```\n",
    "\n",
    "\n",
    "### Scalar Functions with Vector Inputs\n",
    "Sequence is as follows\n",
    "    1. initialize separate values for each variable as a list (make sure all the variables are of same dimension)\n",
    "    2. each derivative calculation will now return a list of derivatives, evaluated at all the separate values initialized earlier (will have the same dimension as the input size)\n",
    "\n",
    "```python\n",
    "    #complex scalar function, multi variable\n",
    "    >>> x = ADnum([2,3])\n",
    "    >>> y = ADnum([3,4])\n",
    "    >>> f = x**3 + y**2\n",
    "    >>> print(f.val)\n",
    "    >>> print(f.der(x))\n",
    "    >>> print(f.der(y))\n",
    "    >>> print(f.gradient)\n",
    "    [17, 43] #f.val\n",
    "    [12, 27] #f.der(x)\n",
    "    [6, 8]   #f.der(y)\n",
    "    [[12, 27], [6, 8]] #f.gradient\n",
    "```\n",
    "\n",
    "Both a and b are ADnum objects, which have the attributes described in the class implementation below.  In particular, a is just an input variable where the function is being evaluated at 2, and b represents a function (in this case the sine function) of the input variable, where the ADmath.sin() has been defined to take an ADnum object as input and output an ADnum object.  Through more complex combinations of basic operations and elementary functions, the user can define any function of the input as an ADnum object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Software Organization\n",
    "We would like to let the user use all numerical operations defined in our AD20 package. The AD20 package contains the `ADnum` module, the `ADmath` module, and the `ADgraph` module.\n",
    "\n",
    "For either a scalar or vector input (either as a numpy array or a list), we will convert the input into an `ADnum` object, which can interact with the other modules. `ADnum` will also contain an overloaded version of basic operations, including addition, subtraction, multiplication, division, and exponentiation, so that the value and derivative are correctly updated after combining ADnum objects through each of these operations.\n",
    "\n",
    "For special functions, we will use `ADmath` to compute the numerical values and the corresponding derivatives. In particular, `ADmath` will contain functions abs, exp, log, sin, cos, and tan.\n",
    "\n",
    "To show a calculation graph, we use `ADgrap`h (and `ADtable`) to show the forward mode calculation process.\n",
    "\n",
    "###  4.1 Directory Structure\n",
    "    AD20/\n",
    "        docs/\n",
    "            Milestone 1.ipynb\n",
    "            Milestone 2.ipynb\n",
    "            requirements.txt\n",
    "            figs/\n",
    "        AD20/\n",
    "            __init__.py\n",
    "                ADnum/\n",
    "                    __init__.py\n",
    "                    ADnum.py\n",
    "                ADmath/\n",
    "                    __init__.py\n",
    "                    ADmath.py\n",
    "                ADgraph/\n",
    "                    __init__.py\n",
    "                    ADgraph.py\n",
    "                    ADtable.py\n",
    "        Tests/\n",
    "            __init__.py\n",
    "            test_AD20.py\n",
    "    README.md\n",
    "    setup.py\n",
    "    LICENSE\n",
    "\n",
    "###  4.2 Modules and Functionality\n",
    "Our package consists of three main modules:\n",
    "\n",
    "- **ADnum:** Contains the `ADnum` class (fully described below).  Create `ADnum` objects, which (inspired by the dual numbers) are defined by the attributes of a value and a derivative, from numbers or tuples.  Define all of the numerical operations for `ADnum` objects, so that they correctly track all derivatives.\n",
    "\n",
    "- **ADmath:** Define elementary functions for `ADnum` objects, correctly tracking all of the derivatives.\n",
    "\n",
    "- **ADgraph:** Create `ADgraph` objects, which can be used to show the computation process in either a graph (ADgraph.py) or table (ADtable.py)\n",
    "\n",
    "###  4.3 Testing and Coverage\n",
    "The tests will be stored in the tests directory (see the repo structure above).  We will use pytest to perform our testing, using `TravisCI` and `Coveralls` for continuous integration and verifying code coverage respectively.\n",
    "\n",
    "###  4.4 Package Distribution\n",
    "We will use `PIP` in `PyPi` to distribute our package. This will allow the user to install the package by using the command\n",
    "\n",
    "    pip install AD20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementation\n",
    "Automatic differentiation will be implemented through the use of `ADnum` objects and building the functions for which we want to take derivatives from these `ADnum` objects as well as the special functions defined for `ADnum` objects in the `ADmath` module.  Each of these functions is itself an `ADnum` object so has an associated value and derivative which was updated when constructing the `ADnum` object through basic operations and special functions.\n",
    "\n",
    "### 5.1 Core Data Structures\n",
    "The main data structure used to represent the functions on which we are performing automatic differentiation will be tuples, with the first entry the value of the ADnum object and the second entry its derivative.  In the case of scalar input, the derivative is also a float.  For vector valued input, the derivative is the gradient of the function, stored as a numpy array.\n",
    "\n",
    "In order to build and store computational graphs in the ADgraph module, we will use a dictionary to represent the graph, where the keys are the nodes of the graph, stored as ADnum objects, and the values associated with each key are the children of that node, stored as lists of ADnum objects.\n",
    "\n",
    "### 5.2 Implemented Classes\n",
    "The main class will be implemented in the `ADnum` module, which will create `ADnum` objects.  It takes as input a single scalar input or a vector input (as either a numpy array or list) and outputs an `ADnum` object.  The `ADnum` objects will store the current value of the function and its derivative as attributes.  By combining simple `ADnum` objects with basic operations and simple functions, we can construct any function we like.  For example,\n",
    "\n",
    "```python\n",
    "#ADnum.py\n",
    "def __mul__(self,other):\n",
    "    try:\n",
    "        return ADnum(self.var*other.val, self.val*other.der+self.der*other.val)\n",
    "    except AttributeError:\n",
    "        return ADnum(self.val*other, other*self.der)\n",
    "    \n",
    "def __pow__(self, other, modulo=None):\n",
    "    try:\n",
    "        return ADnum(self.val**other.val, other.val*(self.val**(other.val-1))*self.der+(self.der**other.val)*np.log(self.val)*other.der)\n",
    "\n",
    "    except AttributeError:# when other is constant\n",
    "        return ADnum(self.val**other, other*(self.val**(other-1))*self.der)\n",
    "```\n",
    "\n",
    "```python\n",
    "    X = AD20.ADnum(4)\n",
    "    Y = AD20.ADnum(0)\n",
    "    F = X + ADmath.sin(Y)\n",
    "```    \n",
    "Where F is now an `ADnum` object, and ADmath.sin() is a specially defined sine function which takes as input an `ADnum` object and returns an `ADnum` object, which allows us to evaluate F and its derivative,\n",
    "\n",
    "```python\n",
    "    F.val = 4\n",
    "    F.der = [1, 1] \n",
    "    X.val = 4\n",
    "    X.der = 1\n",
    "```\n",
    "\n",
    "Notice that F.der now gives the gradient of F with respect to the input `ADnum` objects X and Y.\n",
    "\n",
    "In addition to the sine function used in the example above, the `ADmath` module will also implement the other trigonometric functions, the natural exponential, and the natural logarithm.  All of the functions defined in the `ADmath` module define elementary functions of `ADnum` objects, so that the output is also an `ADnum` object with the val and deriv attributes updated appropriately.  For example,\n",
    "\n",
    "```python\n",
    "#ADmath.py\n",
    "def sin(X):\n",
    "    try:\n",
    "        return adn.ADnum(np.sin(X.val), np.cos(X.val)*X.der)\n",
    "    except AttributeError:\n",
    "        X = adn.ADnum(X, 0)\n",
    "        return sin(X)\n",
    "    \n",
    "def log(X):\n",
    "    try:\n",
    "        return adn.ADnum(np.log(X.val), 1/X.val*X.der)\n",
    "    except AttributeError:\n",
    "        X = adn.ADnum(X, 0)\n",
    "        return log(X)\n",
    "```\n",
    "\n",
    "We will also implement a class, `ADgraph`, for computational graphs.  The constructor takes as input a dictionary, as described above where the keys are nodes and values are the children of the key node. \tThis can then be used to perform forward propagation and could be extended later to include back propagation as an extension of our project.\n",
    " \n",
    "### 5.3 Class Methods and Attributes\n",
    "Each `ADnum` object will have two attributes for the two major functions desired of the class.  The val attribute will be the ADnum object evaluated at the given value and the der attribute will be its derivative. The constructor for this class, sets the value of the object and optionally also sets the value of its derivative,\n",
    "\n",
    "```python\n",
    "#ADnum.py\n",
    "class ADnum():\n",
    "    def __init__(self, a, d = 1):\n",
    "        self.val = a\n",
    "        self.der = d\n",
    "        self.graph = {}\n",
    "```\n",
    "\n",
    "In addition, each `ADnum` object will have a graph attribute, which stores the dictionary which can be used to build a computational graph in the ADgraph class.  The ADnum class will also include methods to overload basic operations, __add__(), __radd__(), __mul__(), __rmul__(), __sub__(), __truedivide__(), and __pow__().  The result of overloading is that the adding, subtracting, multiplying, dividing, or exponentiating two `ADnum` objects returns an ADnum object as well as addition or multiplication by a constant.  For example, Y1, Y2, and Y3 would all be recognized as ADnum objects:\n",
    "\n",
    "```python\n",
    "    X1= ADnum(7)\n",
    "    X2 = ADnum(15)\n",
    "    Y1 = X1+X2\n",
    "    Y2 = X1*X2+X1\n",
    "    Y3 = 5*X1+X2+100\n",
    "```\n",
    "\n",
    "The resulting ADnum objects have both a value and derivative.\n",
    "\n",
    "The `ADgraph` class will be constructed from a dictionary, stored in the attribute dict.  This class will also have an attribute inputs, which stores the nodes which have no parents.  This class will implement a deriv method which returns the derivative from the computational graph.\n",
    "\n",
    "### 5.4 External Dependencies\n",
    "In order to implement the elementary functions, our class will rely on numpyâ€™s implementation of the trigonometric functions, exponential functions, and natural logarithms for evaluation of these special functions, as demonstrated in the definition of the sine function for `ADnum` objects above.\n",
    "\n",
    "We will also use numpy to implement matrix and vector multiplication in cases where the function is either vector valued or takes a vector as an input.\n",
    "\n",
    "### 5.5 Elementary Functions\n",
    "As outlined above, all elementary operations will be defined for `ADnum` objects within the `ADnum` class and we will have a special ADmath module which defines the trigonometric, exponential, and logarithmic functions to be used on ADnum objects, so that they both take as input and return an `ADnum` object, completing the set of defintions of all elementary operations and functions that can be composed to construct more complex functions.\n",
    "\n",
    "### 5.6 What Comes Next?\n",
    "Once this basic implementation is done, we hope to speed up the differentiation process by creating a computational graph that will be capable of storing the intermediate derivatives - this removes the need for the program to recompute derivatives for every distinct value the user passes in. By keeping track of the derivatives at every step and evaluating only when necessary, it will improve the efficiency of the program.\n",
    "\n",
    "Another implementation may be the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
