{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Example Usage Including Imports for a Scalar Function of a Scalar Variable\n",
    "\n",
    "Here we show several examples of scalar functions of scalar variables, including an example of using our package to implement Newton's Method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiwhanyoon/Desktop/cs207/cs207-FinalProject\n",
      "/Users/jiwhanyoon/Desktop/cs207/cs207-FinalProject/AD20\n"
     ]
    }
   ],
   "source": [
    "#commands to change to the \n",
    "%pwd \n",
    "%cd ..\n",
    "%cd AD20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n",
      "[12.  2.]\n",
      "3.0\n",
      "[1. 0.]\n",
      "4.0\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#necessary imports\n",
    "import AD20\n",
    "import numpy as np\n",
    "from AD20.ADnum_multivar import ADnum\n",
    "from AD20 import ADmath_multivar as ADmath\n",
    "\n",
    "x = ADnum(3, ins = 2, ind = 0) # Step 1: intialize x to a specific value\n",
    "y = ADnum(4, ins = 2, ind= 1)\n",
    "f = 2 * y + 2*x**2\n",
    "# f = 2 * x # Step 2: write a function which we would like to take the derivative\n",
    "\n",
    "# Steps 3 and 4: Use the class attributes to access the value and deriviative of the function at the value of the input x \n",
    "\n",
    "print(f.val) #should equal 81\n",
    "print(f.der) #should equal 72\n",
    "print(x.val) #should equal 3\n",
    "print(x.der) #should equal 1\n",
    "print(y.val)\n",
    "print(y.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2246467991473532e-16\n",
      "-1.0\n",
      "3.141592653589793\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#another example with a trignometric function\n",
    "x = ADnum(np.pi, der = 1) # Step 1: initialize x, this time at pi\n",
    "f = ADmath.sin(x) # Step 2: create a function, using elementary functions from the ADmath module\n",
    "\n",
    "#Steps 3 and 4: Use the class attributes to access the value and derivative\n",
    "print(f.val) # should print 1.22e-16 due to floating point error in numpy implementation (should be 0)\n",
    "print(f.der) # should print -1.0\n",
    "print(x.val) # should print 3.14\n",
    "print(x.der) # should print 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to easily be able to access the value and derivative of a function at many different points.  As an alternative to the method for defining `f` in the previous two examples, we could define `f` as a python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.718281828459045 3.718281828459045\n",
      "3.718281828459045 3.718281828459045\n"
     ]
    }
   ],
   "source": [
    "#example to easily access value and derivative at multiple points by defining f as a function\n",
    "def f(x):\n",
    "    return x+ADmath.exp(x)\n",
    "\n",
    "#get the value and derivative at 0\n",
    "y = ADnum(1, der = 1)\n",
    "print(f(y).val, f(y).der)\n",
    "\n",
    "#an alternate approach to get the value and derivative at 1\n",
    "print(f(ADnum(1, der = 1)).val, f(ADnum(1, der = 1)).der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the above example, we required the natural exponential, an elementary function, to be used from the ADmath package, so that f may take as input and return an ADnum object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Newton's Method for a Scalar Valued Function\n",
    "One basic application of differentiation is Newton's method for finding roots of a function.  For demonstration of using our package for such an application, we will consider the function\n",
    "$$f(x) = x^2 + \\sin(x)$$\n",
    "which we know has a root at $x=0$.  The plot below also shows that the function has an additional root near -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-2.5, 2.55, 1000)\n",
    "f = x**2+np.sin(x)\n",
    "\n",
    "plt.plot(x, f, linewidth = 2)\n",
    "plt.plot(x, np.zeros((1000,)), '--')\n",
    "plt.xlabel('x', fontsize = 16)\n",
    "plt.ylabel('f(x)', fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize =14)\n",
    "plt.title('Plot of f(x) Showing Two Roots', fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of Newton's method using AD20, without hardcoding the derivative\n",
    "\n",
    "#function that we wish to find the roots of\n",
    "def f(x):\n",
    "    return x**2+ADmath.sin(x)\n",
    "\n",
    "#Newton's method\n",
    "x = ADnum(1) #set an initial guess for the root\n",
    "\n",
    "for i in range(1000):\n",
    "    dx = -f(x).val/f(x).der #get change using ADnum attributes\n",
    "    if np.abs(dx) < .000001: #check if within some tolerance\n",
    "        print('Root found at:' + str(x.val))\n",
    "        break\n",
    "    x = x+dx #update the guess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we found the root at zero.  Using a different initialization point, we can find the other root of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ADnum(-1) #set an initial guess for the root\n",
    "\n",
    "for i in range(1000):\n",
    "    dy = -f(y).val/f(y).der #get change using ADnum attributes\n",
    "    if np.abs(dy) < .000001: #check if within some tolerance\n",
    "        print('Root found at:' + str(y.val))\n",
    "        break\n",
    "    y = y+dy #update the guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Future Extensions of the Basic Scalar Implementation\n",
    "In the future, we will extend our basic package to find the gradient of scalar valued functions of multiple variables and the Jacobian of vector valued functions of vector valued input.  The following gives an outline of the usage of this future work.\n",
    "\n",
    "\n",
    "### Functions of Multiple Variables\n",
    "In case case with a function with more than one variable, the sequence is similar except when creating ADnum objects, the user must specify the total number of input variables, and the index of each variable in the gradient (so the the constructor of the ADnum class can correctly assign the derivative of the input variable):\n",
    "    1. initialize each variable to a specific value where the function should be evaluated\n",
    "    2. return the gradient as a numpy array when `f.der` is called \n",
    "    \n",
    "```python\n",
    "    # scalar function, multi variable\n",
    "    >>> x = ADnum(2, ins = 2, ind = 0)\n",
    "    >>> y = ADnum(3, ins = 2, ind = 1)\n",
    "    >>> f = 3 * x**3 + 2 * y**3\n",
    "    >>> print(f.val)\n",
    "    >>> print(f.der)\n",
    "    >>> print(x.val)\n",
    "    >>> print(x.der)\n",
    "    >>> print(y.val)\n",
    "    >>> print(y.der)\n",
    "    78\n",
    "    np.array([36, 54])\n",
    "    2\n",
    "    np.array([1, 0])\n",
    "    3\n",
    "    np.array([0, 1])\n",
    "```\n",
    "\n",
    "### Vector-valued Functions\n",
    "Each component of a vector valued function is just a scalar valued function of one or more input variables.  Thus, we can easily combine the previous results to get the Jacobian of a vector valued function.  By updating our methods to broadcast appropriately for an array, we can easily access these attributes,\n",
    "\n",
    "```python\n",
    "    >>> x = ADnum(2, ins = 2, ind = 0)\n",
    "    >>> y = ADnum(3, ins = 2, ind = 0)\n",
    "    >>> F = [x**2, x+y, 4*y]\n",
    "    >>> F[1].der = [1 , 1]\n",
    "    >>> F.der = [[4, 0], [1, 1], [0, 4]]\n",
    "    >>> F[0].val = 4\n",
    "    >>> F.val = [4, 5, 12]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Software Organization\n",
    "We would like to let the user use all numerical operations defined in our AD20 package. The AD20 package contains the `ADnum` module, the `ADmath` module, and the `ADgraph` module.\n",
    "\n",
    "For either a scalar or vector input (either as a numpy array or a list), we will convert the input into an `ADnum` object, which can interact with the other modules. `ADnum` will also contain an overloaded version of basic operations, including addition, subtraction, multiplication, division, and exponentiation, so that the value and derivative are correctly updated after combining ADnum objects through each of these operations.\n",
    "\n",
    "For special functions, we will use `ADmath` to compute the numerical values and the corresponding derivatives. In particular, `ADmath` will contain functions abs, exp, log, sin, cos, and tan.\n",
    "\n",
    "To show a calculation graph, we use `ADgrap`h (and `ADtable`) to show the forward mode calculation process.\n",
    "\n",
    "###  4.1 Directory Structure\n",
    "    AD20/\n",
    "        AD20/\n",
    "            __init__.py\n",
    "            ADnum.py\n",
    "            ADmath.py\n",
    "\n",
    "        Tests/\n",
    "            __init__.py\n",
    "            test_AD20.py\n",
    "    docs/\n",
    "        Milestone 1.ipynb\n",
    "        Milestone 2.ipynb\n",
    "        figs/\n",
    "    README.md\n",
    "    setup.cfg\n",
    "    requirements.txt\n",
    "    LICENSE\n",
    "\n",
    "###  4.2 Modules and Functionality\n",
    "Our package consists of three main modules:\n",
    "\n",
    "- **ADnum:** Contains the `ADnum` class (fully described below).  Create `ADnum` objects, which (inspired by the dual numbers) are defined by the attributes of a value and a derivative, from numbers or tuples.  Define all of the numerical operations for `ADnum` objects, so that they correctly track all derivatives.\n",
    "\n",
    "- **ADmath:** Define elementary functions for `ADnum` objects, correctly tracking all of the derivatives.\n",
    "\n",
    "and in future implementation,\n",
    "\n",
    "- **ADgraph:** Create `ADgraph` objects, which can be used to show the computation process in either a graph (ADgraph.py) or table (ADtable.py)\n",
    "\n",
    "###  4.3 Testing and Coverage\n",
    "All tests are contained in the test_AD20.py file in the tests directory (see the repo structure above).  We will use pytest to perform our testing, using `TravisCI` and `Coveralls` for continuous integration and verifying code coverage respectively.  The test suite contains unit tests for all of the class methods implemented in ADnum and all the elementary functions implemented in ADmath.  This suite also contains several functions which are composed of several different operations and elementary functions for more advanced testing.\n",
    "\n",
    "###  4.4 Package Distribution\n",
    "For the final project submission, we will use `PIP` in `PyPi` to distribute our package. This will allow the user to install the package by using the command\n",
    "\n",
    "    pip install AD20\n",
    "    \n",
    "Note that the current method for installing `AD20` through git is outlined in user interaction with numpy as the only current external dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementation\n",
    "Automatic differentiation is implemented through the use of `ADnum` objects and building the functions for which we want to take derivatives from these `ADnum` objects as well as the special elementary functions defined for `ADnum` objects in the `ADmath` module.  Each of these functions is itself an `ADnum` object so has an associated value and derivative which was updated when constructing the `ADnum` object through basic operations and elementary functions.\n",
    "\n",
    "### 5.1 Core Data Structures\n",
    "`ADnum` objects can be thought of as a tuple, where the first entry is the value and the second entry is the derivative.  Each of these attributes is either a scalar or a numpy array for ease of computation.  In the case of scalar input, the derivative is also a scalar.  For vector valued input, the derivative is the gradient of the function, stored as a numpy array.\n",
    "\n",
    "In order to build and store computational graphs in the ADgraph module, we will use a dictionary to represent the graph, where the keys are the nodes of the graph, stored as `ADnum` objects, and the values associated with each key are the children of that node, stored as lists of ADnum objects.\n",
    "\n",
    "### 5.2 Implemented Classes, Methods, and Attributes\n",
    "The main class is the `ADnum` module, which is used to create `ADnum` objects.  It takes as input a single scalar input or a vector input (as a numpy array) and outputs an `ADnum` object.  The `ADnum` objects store the current value of the function and its derivative as attributes. \n",
    "\n",
    "These two attributes represent the two major functionalities desired of the class.  The `val` attribute is the ADnum object evaluated at the given value and the `der` attribute is its derivative at the given value. The constructor for this class, sets the value of the object and optionally also sets the value of its derivative,\n",
    "\n",
    "```python\n",
    "#ADnum.py\n",
    "class ADnum():\n",
    "    def __init__(self, a, d = 1):\n",
    "        self.val = a\n",
    "        self.der = d\n",
    "        self.graph = {} #for future implementation\n",
    "```\n",
    "\n",
    "The `ADnum` class also includes methods to overload basic operations, __add__(), __radd__(), __mul__(), __rmul__(), __sub__(), __rsub__(), __truediv__(), __rtruediv__(), __pow__(), and __rpow__().  The result of overloading is that the adding, subtracting, multiplying, dividing, or exponentiating two `ADnum` objects returns an `ADnum` object as well as addition or multiplication by a constant.  For example, Y1, Y2, and Y3 are all recognized as `ADnum` objects:\n",
    "\n",
    "```python\n",
    "    X1= ADnum(7)\n",
    "    X2 = ADnum(15)\n",
    "    Y1 = X1+X2\n",
    "    Y2 = X1*X2+X1\n",
    "    Y3 = 5*X1+X2+100\n",
    "```\n",
    "\n",
    "The resulting ADnum objects have both a value and derivative.  An example overloaded function is the following:\n",
    "\n",
    "\n",
    "```python\n",
    "#ADnum.py\n",
    "def __mul__(self,other):\n",
    "        try:\n",
    "            return ADnum(self.val*other.val, self.val*other.der+self.der*other.val)\n",
    "        except AttributeError:\n",
    "            other = ADnum(other, 0)\n",
    "            return self*other\n",
    "```\n",
    "\n",
    "By combining simple `ADnum` objects with basic operations and simple functions, we can construct any function we like.\n",
    "\n",
    "```python\n",
    "    X = ADnum(4)\n",
    "    F = X + ADmath.sin(4-x)\n",
    "```    \n",
    "Where F is now an `ADnum` object, and ADmath.sin() is a specially defined sine function which takes as input an `ADnum` object and returns an `ADnum` object, which allows us to evaluate F and its derivative,\n",
    "\n",
    "```python\n",
    "    F.val = 4\n",
    "    F.der = 0\n",
    "    X.val = 4\n",
    "    X.der = 1\n",
    "```\n",
    "\n",
    "In addition to the sine function used in the example above, the `ADmath` module also implements the trigonometric functions: `sin()`, `cos()`, `tan()`, `csc()`, `sec()`, `cot()`, the inverse trigonometric functions: `arcsin()`, `arccos()`, `arctan()`, the hyperbolic trig functions: `sinh()`, `cosh()`, `tanh()`, and the natural exponential `exp()` and natural logarithm `log()`.  All of the functions defined in the `ADmath` module define elementary functions of `ADnum` objects, so that the output is also an `ADnum` object with the val and deriv attributes updated appropriately.  For example,\n",
    "\n",
    "```python\n",
    "#ADmath.py\n",
    "def sin(X):\n",
    "    try:\n",
    "        return adn.ADnum(np.sin(X.val), np.cos(X.val)*X.der)\n",
    "    except AttributeError:\n",
    "        X = adn.ADnum(X, 0)\n",
    "        return sin(X)\n",
    "    \n",
    "def log(X):\n",
    "    try:\n",
    "        return adn.ADnum(np.log(X.val), 1/X.val*X.der)\n",
    "    except AttributeError:\n",
    "        X = adn.ADnum(X, 0)\n",
    "        return log(X)\n",
    "```\n",
    "\n",
    "We will also implement a class, `ADgraph`, for computational graphs.  The constructor takes as input a dictionary, as described above where the keys are nodes and values are the children of the key node.  The `ADgraph` class will be constructed from a dictionary, stored in the attribute dict.  This class will also have an attribute inputs, which stores the nodes which have no parents.  This class will implement methods to display the computational graphs and tables used to compute the derivatives of the `ADnum` objects.\n",
    "\n",
    "### 5.3 External Dependencies\n",
    "In order to implement the elementary functions, our ADmath relies on numpyâ€™s implementation of the trigonometric functions, exponential functions, and natural logarithms for evaluation of these special functions, as demonstrated in the definition of the sine function for `ADnum` objects above.\n",
    "\n",
    "We will also use numpy to implement matrix and vector multiplication in cases where the function is either vector valued or takes a vector as an input.\n",
    "\n",
    "### 5.4 Elementary Functions\n",
    "As outlined above, all elementary operations are defined for `ADnum` objects within the `ADnum` class and we have a special `ADmath` module which defines the trigonometric, exponential, and logarithmic functions to be used on ADnum objects, so that they both take as input and return an `ADnum` object, completing the set of defintions of all elementary operations and functions that can be composed to construct more complex functions.\n",
    "\n",
    "### 5.5 Future Implementation\n",
    "Our current implementation performs automatic differentiation for scalar functions of scalar variables.  Our constructor needs to be modified for functions of multiple inputs so that the `der` attribute will now be represented by a numpy array in these cases.  Minor revisions will also need to be applied to the methods defined in the `ADnum` and `ADmath` classes to ensure that these multidimensional `der` attributes are correctly updated, using the correct elementwise matrix multiplication for the numpy arrays.\n",
    "\n",
    "We will also need to implement the functionality described in section 6, which includes an implementation of reverse mode for backpropagation and an implementation of the ADgraph class for building computational graphs and tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Project Extension\n",
    "\n",
    "In order to expand our project from the basic forward mode automatic differentiation, we will make two additional developments for pedagogical and application purposes.\n",
    "\n",
    "### 6.1 Computational Graphs and Tables\n",
    "We will implement the class `ADgraph` which stores the computational graph used to compute the derivatives in forward mode.  For every operation we create an additional node which represents another trace in the program.  This graph will be stored as an attribute of the `ADnum` objects.  For pedagogical purposes, we want to be able to visualize this process so we will use external packages for displaying graphs where the edge labels display the corresponding operation.  Correspondingly, we will also develop the functionality to display a table showing the trace, elementary operation, value, and derivative at each step.  Such a tool could be useful in the classroom for teaching students how automatic differentiation works.\n",
    "\n",
    "This will require modifying all of our methods to correctly add to the dictionary which contains the computational graph information for each operation that we have previously overloaded.  This will also involve the added challenge of ensuring compatability between our program and an external program for visualizing graphs and tables.\n",
    "\n",
    "### 6.2 Backpropagation\n",
    "The computational graph is also necessary to implement the reverse mode of automatic differentiation, which is important to many applications.  For example, backpropagation is the underlying method to fit the parameters of neural networks.  We will use the computational graph to store the structure needed to backpropagate derivatives.  Thus, we will have expanded our project to perform both forward and reverse automatic differentiation, making the package more suitable to a variety of applications. \n",
    "\n",
    "To perform backpropagation requires writing methods to correctly traverse the computational graph and propagate derivatives from the output through the intermediates to the inputs, which will require developing new methods for the `ADgraph`class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
